{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6FTphYjtNR7",
        "outputId": "ae7d7b06-26ef-4ac7-8b7d-7234f2da2d05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.1.2-py3-none-any.whl (803 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.6/803.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai\n",
            "  Downloading openai-1.9.0-py3-none-any.whl (223 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.4/223.4 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting weaviate-client\n",
            "  Downloading weaviate_client-3.26.2-py3-none-any.whl (120 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.4/120.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.24)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.14 (from langchain)\n",
            "  Downloading langchain_community-0.0.14-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2,>=0.1.14 (from langchain)\n",
            "  Downloading langchain_core-0.1.14-py3-none-any.whl (229 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.5/229.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.0.84,>=0.0.83 (from langchain)\n",
            "  Downloading langsmith-0.0.83-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m652.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Collecting typing-extensions<5,>=4.7 (from openai)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Collecting validators<1.0.0,>=0.21.2 (from weaviate-client)\n",
            "  Downloading validators-0.22.0-py3-none-any.whl (26 kB)\n",
            "Collecting authlib<2.0.0,>=1.2.1 (from weaviate-client)\n",
            "  Downloading Authlib-1.3.0-py2.py3-none-any.whl (223 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.7/223.7 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.10/dist-packages (from authlib<2.0.0,>=1.2.1->weaviate-client) (41.0.7)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.14->langchain) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography->authlib<2.0.0,>=1.2.1->weaviate-client) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography->authlib<2.0.0,>=1.2.1->weaviate-client) (2.21)\n",
            "Installing collected packages: validators, typing-extensions, mypy-extensions, marshmallow, jsonpointer, h11, typing-inspect, jsonpatch, httpcore, langsmith, httpx, dataclasses-json, authlib, weaviate-client, openai, langchain-core, langchain-community, langchain\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed authlib-1.3.0 dataclasses-json-0.6.3 h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.2 langchain-community-0.0.14 langchain-core-0.1.14 langsmith-0.0.83 marshmallow-3.20.2 mypy-extensions-1.0.0 openai-1.9.0 typing-extensions-4.9.0 typing-inspect-0.9.0 validators-0.22.0 weaviate-client-3.26.2\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain openai weaviate-client"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOwaYXFlttv5",
        "outputId": "ff0b8006-3446-4412-940d-9d987fdf91b8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDqL0kb0w5hn",
        "outputId": "a3e70a7d-bdc0-4f85-e699-70102273ebd1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-4.0.0-py3-none-any.whl (283 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/283.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.9/283.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.9/283.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-4.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kyJhYclxbdM",
        "outputId": "63ded855-c3b2-4021-ec7b-853e7de321df"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.11.17)\n",
            "Installing collected packages: tiktoken\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tiktoken-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "OPENAI_API_KEY= userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "f324gpHAtYUB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dotenv\n",
        "dotenv.load_dotenv()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EadTDU-tkNh",
        "outputId": "d8eabb24-368a-4467-cf67-f6605e1e90c5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "loader=PyPDFDirectoryLoader(\"documents/\")"
      ],
      "metadata": {
        "id": "oBci4M1Stsrk"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=loader.load()"
      ],
      "metadata": {
        "id": "5fD40tJcuSXU"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qz-zK0p5wrOh",
        "outputId": "031313a5-a366-41ec-ba96-15223f30d03e"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='1/23/24, 3:23 PM Large language model - Wikipedia\\nhttps://en.wikipedia.org/wiki/Large_language_model 1/21\\nAn illustration of main components of the\\ntransformer model from the original paper,\\nwhere layers were normalized after (instead of\\nbefore) multiheaded attention.Large language model\\nA large language model  (LLM ) is a language model  nota ble for its ability to achieve gene ral-purpose language generation.\\nLLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive\\nself-supervised  and semi-supervised  training process.[1] LLMs are artificial neural networks  typically built with a\\ntransformer -based architecture. Some recent implementations are based on alternative architectures such as recurrent\\nneural network  variants and Mamba  (a state space  model).[2][3][4]\\nLLMs can be used for text generation, a form of generative AI, by taking an input text and repeatedly predicting the next\\ntoken or word.[5] Up to 2020, fine tunin g was the only way a model could be adapted to be able to accomplish specific tasks.\\nLarger sized models, such as GPT-3 , however , can be prompt-engineered  to achieve similar  results.[6] They are thought  to\\nacquire knowledge about syntax, semantics and \"ontology\" inherent in human language corpora, but also inaccuracies and\\nbiases  present in the corpora.[7]\\nSome notable LLMs are OpenAI \\'s GPT  mod els (e.g., GPT-3.5  and GPT-4 , used in ChatGPT ), Google \\'s PaLM  and Gemini\\n(used in Bard ), Microsoft\\'s Copilot , Meta \\'s LLaMA  family of open-source models, and Anthropic \\'s Claude  models.\\nAt the 2017 NeurIPS  conference, Google researchers pres ented their landmark\\npaper \"Attention Is All You Need \", which, with the goal of improving upon 2014\\nSeq2seq  technolo gy, intro duced the transformer archit ecture ,[8] based mainly\\non the attention  mechani sm, developed by Bahdanau et. al. in 2014.[9] The\\nfollowing year in 2018, BERT  was introduced , which quickly became\\n\"ubiquitous\".[10]\\nAlthough GPT-1  was introduced in 2018, it was GPT-2  in 2019 that caught\\nwidespread attention because OpenAI  at first deemed it too powerful to release\\npublicly, out of fear of malicious use.[11] GPT-3  in 2020 went a step further and\\nas of 2024  is available only via API with no offering of downloading the model\\nto execute locally. But it was the 2022 consumer-facing browser -based\\nChatGPT  that captured the imaginations of the general population and\\n\"completely changed the world\".[12] The 2023 GPT-4  was praised for its\\nincreased accurac y and as a \"holy grail\" for its multimodal  capabilities.[13]\\nOpenAI did not reveal high-level architecture and the number of parameters  of\\nGPT-4.\\nIn the meantime, competing language models have for the most part been\\nplaying catch-up to the GPT series, at least in terms of number of\\nparameters.[14] Notable exceptions in terms of number of parameters included Google\\'s 2019 T5-11B  and 2022 PaLM-E .\\nSince 2022, open source mod els have been  gaining popularity, especially at first with BLOOM  and LLaMA , though the latter\\nis restrict ed to only noncommercial uses. Mistral AI\\'s models  Mistral 7B and Mixtral 8x7b have the more permissive Apache\\nLicense . As of January 2024 , Mixtral 8x7b is the most powerful open source LLM according to the LMSYS Chatbot Arena\\nLeaderboard, being more powerful than GPT-3.5 but not as powerful as GPT-4.[15]\\nUsing a modification of byte-pair encoding , in the first step, all unique characte rs (including blanks and punctuation marks )\\nare treated as an initial set of n-grams  (i.e. initial set of uni-grams). Successively the most frequent pair of adjacent\\ncharacters is merged into a bi-gram  and all instances of the pair are replaced by it. All occurrences of adjacent pairs of\\n(previously merged) n-grams that most frequently occur together are then again merged into even lengthier n-gram\\nrepeatedly until a vocabulary of prescribed size is obtained (in case of GPT-3 , the size is 50257) .[16] Token vocabulary  consists\\nof integers , spannin g from zero up to the size of the token vocabulary. New words can always be interpreted as combinations\\nof the tokens and the initial-set uni-grams.[17]\\nA token vocabulary based on the frequencies extracted from mainly English corpora uses as few tokens as possible for an\\naverage English word. An average word in another language encoded by such an English-optimized tokenizer is however split\\ninto suboptimal amount of tokens.\\ntokenizer: texts -> series of numerical \"tokens\" may be split into:History\\nDataset preprocessing\\nProbabilistic tokenization', metadata={'source': 'documents/Large language model - Wikipedia.pdf', 'page': 0}),\n",
              " Document(page_content='1/23/24, 3:23 PM Large language model - Wikipedia\\nhttps://en.wikipedia.org/wiki/Large_language_model 2/21n-grams: token izer: texts-> seriesof numerical\" t ok ens\"\\nnumbers as \"tokens\":300017509251339946132168 28629052 366834826411\\nProbabilistic tokenization also compresses  the datas ets, which is the reason for using the byte pair encoding  algo rithm as a\\ntokenizer. Because LLMs generally require input to be an array that is not jagged , the shorter texts must be \"padded\"  until\\nthey matc h the length of the longest one. How many tokens are, on avera ge, needed per word depends on the language of the\\ndataset.[18][19]\\nIn the context of training LLMs, datasets are typically cleaned by remov ing toxic passages from the dataset, discarding low-\\nquality data, and de-duplication.[20] Clea ned datasets can increase training efficiency and lead to improve d downstream\\nperformance.[21][22]\\nWith the increasin g proportion of LLM-generated content on the web, data cleaning in the future may include filtering out\\nsuch content. LLM-generated conten t can pose a problem if the content is similar to human text (making filtering difficult)\\nbut of lower quality (degrading performance of models trained on it).[23]\\nReinforcement learning from human feedback  (RLHF) through algorithms, such as proximal policy optimization , is used to\\nfurther fine-tune a model based on a dataset of human preferences.[24]\\nUsing \"self-instruct\" approaches, LLMs have been able to bootstrap  corr ect responses, replacing any naive responses,\\nstarting from human-generated corr ections of a few cases. For example, in the instruction \"Write an essay about the main\\nthemes represented in Hamlet,\" an initial naive completion might be \\'If you submit the essay after March 17, your grade will\\nbe reduced by 10% for each day of delay,\" based on the frequency of this textual sequence in the corpus.[25]\\nThe largest LLM may be too expensi ve to train and use directly. For such  models, mixture of experts  (MoE) can be applied, a\\nline of research pursued by Google researchers since 2017 to train models reaching up to 1 trillion parameters.[26][27][28]\\nMost results previously achievable only by (costly) fine-tuning, can be achieved through prompt engineerin g, althoug h\\nlimited to the scope of a single conversation (more precisely, limited to the scope of a context window).[29]\\nIn order to find out which tokens are relevant to each other within the scope of the context window, the attention mechanism\\ncalculates \"soft\" weights for each token, more precisely for its embedding, by using multiple attention heads, each with its\\nown \"rele vance\" for calculating its own soft weights. For example, the small (i.e. 117M parameter sized) GPT-2  model, has\\nhad twelve attention heads and a context window of only 1k token.[31] In its medium version it has 345M parameters and\\ncontains 24 layers, each with 12 attention heads. For the training with gradient descent a batch size of 512 was utilized.[17]\\nThe largest models can have a context window sized up to 200k (for example, Claude 2.1).[32] Other models with large\\ncontext windows includes GPT-4 Turbo, with a context window of up to 128k tokens.[33] Note that this maximum refers to\\nthe number of input tokens and that the maximum number of output tokens differs from the input and is often smaller. For\\nexample, the GPT-4 Turbo model has a maximum output of 4096 tokens. Also, As of January 2024 , GPT -4 Turbo is, for all\\ntiers of service, \"currently under preview with restrictive rate limits  that make them suitable for testing and evaluations, but\\nnot for production usage\".[34]\\nLength of a conversation that the model can take into account when generating its next answer is limited by the size of a\\ncontext window, as well. If the length of a conversation, for example with Chat-GPT , is longer than its context window, only\\nthe parts inside the context window are taken into account when generating the next answer, or the model needs to apply\\nsome algorithm to summarize the too distant parts of conversation.\\nThe short comings of making a context window larger include higher computational cost and possibly diluting the focus on\\nlocal context, while making it smalle r can cause a model to miss an important long-range dependency. Balancing them are a\\nmatter of experimentation and domain-specific considerations.Dataset cleaning\\nTraining and architecture\\nReinforcement learning from human feedback (RLHF)\\nInstruction tuning\\nMixture of experts\\nPrompt engineering, attention mechanism, and context window', metadata={'source': 'documents/Large language model - Wikipedia.pdf', 'page': 1}),\n",
              " Document(page_content='1/23/24, 3:23 PM Large language model - Wikipedia\\nhttps://en.wikipedia.org/wiki/Large_language_model 3/21\\nWhen each head calculates, according to its own\\ncriteria, how much other tokens are relevant for\\nthe \"it_\" token, note that the second attention\\nhead, represented by the second column, is\\nfocusing most on the first two rows, i.e. the tokens\\n\"The\" and \"animal\", while the third column is\\nfocusing most on the bottom two rows, i.e. on\\n\"tired\", which has been tokenized into two\\ntokens.[30]A model may be pre-trained either to predict how the segment continu es, or\\nwhat is missing in the segment, given a segment from its training dataset.[35]\\nIt can be either\\nautoregressive (i.e. predicting how the segment continues, the way GPTs\\ndo it): for example given a segment \"I like to eat\", the model predicts \"ice\\ncream\", or \"sushi\".\\n\"masked \" (i.e. filling in the parts missing from the segment, the way\\n\"BER T\"[36] does it): for example, given a segment \"I like to [__] [__]\\ncream\", the model predicts that \"eat\" and \"ice\" are missing.\\nModels may be trained on auxiliary tasks which test their understanding of\\nthe data distribution, such as Next Sentence Prediction (NSP), in which pairs\\nof sentences are presented and the model must predict whether they appear\\nconsecutively in the training corpus.[36] During training, regularization  loss is\\nalso used  to stabilize training. However regularization loss is usually not used\\nduring testing  and evaluation.\\nAdvances in software and hardware have reduced the cost substantially since\\n2020, such that in 2023 training of a 12-billion-parameter LLM\\ncomputational cost is 72,300 A100-GPU -hours, while in 2020 the cost of\\ntraining a 1.5-billion-parameter LLM (which was two orders of magnitude\\nsmaller than the state of the art in 2020) was between $80 thousand and $1.6\\nmillion.[37][38][39] Since 2020, large  sums were invested in increasingly large\\nmodels. For example, training of the GPT-2 (i.e. a 1.5-billion-parameters\\nmodel) in 2019 cost $50,000, while training of the PaLM (i.e. a 540-billion-\\nparameters model) in 2022 cost $8 million.[40]\\nFor Transformer-b ased LLM, training cost is much higher than inference  cost.\\nIt costs 6 FLOPs  per parameter to train on one token, whereas it costs 1 to 2\\nFLOPs per parameter to infer on one token.[41]\\nThere are certain tasks that, in principle, cannot be solved by any LLM , at least not without the use of external tools or\\nadditional software. An example of such a task is responding to the user\\'s input \\'354 * 139 = \\', provided that the LLM has not\\nalready encountered a continuation  of this calculation in its training corpus. In such cases, the LLM needs to resort to\\nrunning program code that calculate s the result, which can then be inclu ded in its response. Another example is \\'What is the\\ntime now ? It is \\', where a separate program interpreter would need to execute a code to get system time on the computer, so\\nLLM could include it in its reply.[42][43] This basic strategy can be sophisticated with multiple attempts of generated\\nprograms, and other sampling strategies.[44]\\nGenerally, in order to get an LLM to use tools, one must finetune it for tool-use. If the number of tools is finite, then\\nfinetuning may be done just once. If the number of tools can grow arbitrarily, as with online API services,  then the LLM can\\nbe finetuned to be able to read API documentation and call API correctly.[45][46]\\nA simpler  form of tool use is Retrieval Augmen ted Generation : augmen t an LLM with document retrieva l, sometim es using a\\nvector database . Given a query, a document retriever is called to retrieve the most relevant (usually measured by first\\nencoding the query and the documents into vectors, then finding the documents with vectors closest in Euclidean norm to\\nthe query vector). The LLM then generates an output based on both the query and the retrieved documents.[47]\\nAn LLM is a language model, which is not an agent as it has no goal, but it can be used as a component of an intelligent\\nagent .[48] Researchers have described several methods for such integrations.\\nThe ReAct (\"Reason + Act\") method constructs an agent  out of an LLM, using the LLM as a planner. The LLM is prompted to\\n\"think out loud\". Specifically, the language model is prompted with a textual description of the environment, a goal, a list of\\npossible actions, and a record of the actions and observations so far. It generates one or more thoughts before generating an\\naction, which is then executed in the environment.[49] The linguistic description of the environment given to the LLM\\nplanner can even be the LaTeX code of a paper describing the environment.[50]Training cost\\nTool use\\nAgency', metadata={'source': 'documents/Large language model - Wikipedia.pdf', 'page': 2}),\n",
              " Document(page_content='1/23/24, 3:23 PM Large language model - Wikipedia\\nhttps://en.wikipedia.org/wiki/Large_language_model 4/21In the DEPS (\"De scribe, Explain, Plan and Select\") method, an LLM is first connected to the visual world via image\\ndescriptions, then  it is prompted to produce plans for complex tasks and behaviors based on its pretrained knowledge and\\nenvironmental feedback it receives.[51]\\nThe Refle xion method[52] construc ts an agent that learns over multiple episodes. At the end of each  episode, the LLM is given\\nthe recor d of the episode, and prompted to think up \"lessons learned\", which would help it perform better at a subsequent\\nepisode. These \"lessons learned\" are given to the agent in the subsequent episodes.\\nMonte Carlo tree search  can use an LLM as rollout heuristic.  When a programmatic world model is not available, an LLM can\\nalso be prompted with a description of the environment to act as world model.[53]\\nFor open-ended exploration, an LLM can be used to score observations for their \"interestingness\", which can be used as a\\nreward signal to guide a normal (non-LLM) reinforcement learning agent.[54] Alternatively, it can propose increasingly\\ndifficult tasks  for curriculum learnin g.[55] Instead of outpu tting individual actions, an LLM planner can also construct\\n\"skills\", or functions  for comp lex action sequences. The skills can be stored and later invoke d, allowing increasing levels of\\nabstraction in planning.[55]\\nLLM-powered agents can keep a long-term memory of its previous contexts, and the memory can be retrieved in the same\\nway as Retrieval Augmented Generation. Multiple such agents can interact socially.[56]\\nTypically, LLM are trained with full-  or half-precision floating point numbers (float32 and float16). One float16 has 16 bits, or\\n2 bytes, and so one billion parameters require 2 gigabytes. The largest models typically have 100 billion parameters,\\nrequiring 200 gigabytes to load, which places them outside the range of most consumer electronics.\\nPost-training quantization[57] aims to decrease the space requirement by lowering precision of the parameters of a trained\\nmodel, while preserving most of its performance.[58][59] The simp lest form of quantization simply truncates all numbers to a\\ngiven number of bits. It can be improved by using a different quantizatio n codebook  per layer. Further  improvement can be\\ndone by applying different precisions  to different parameters, with higher precision for particularly important parameters\\n(\"outlier weights\").[60]\\nWhile quantized models are typically frozen, and only pre-quantized models are finetuned, quantized models can still be\\nfinetuned.[61]\\nMultimodality means \"having severa l modalities\", and a \"modality\"  refers to a type of input or output, such as video, image,\\naudio, text, proprioception , etc.[62] There have been many AI models trained specifically to ingest one mod ality and output\\nanother modality, such as AlexNet  for image to label,[63] visual question answering  for image-text to text,[64] and speech\\nrecognition  for speech to text.\\nA com mon method to create multim odal models out of an LLM is to \"tokenize\" the output of a trained encoder. Concretely,\\none can construct a LLM that can understand images as follows: take a trained LLM, and take a trained image encoder .\\nMake a small multilayered perceptron , so that for any image , the post-processed vector  has the same\\ndimensions as an encoded token. That is an \"image token\". Then, one can interleave text tokens and image tokens. The\\ncompound model is then finetuned on an image-text dataset. This basic construction can be applied with more sophistication\\nto improve the model. The image encoder may be frozen to improve stability.[65]\\nFlamingo demonstrated the effectiv eness of the tokenization method, finetuning a pair of pretrained language model and\\nimage encoder to perform better on visual question answering than mode ls trained from scratch.[66] Google PaLM  model was\\nfinetuned into a multimodal mode l PaLM-E using the tokenization method, and applied to robotic control.[67] LLaMA\\nmodels have also been turned multimodal using the tokenization method, to allow image inputs,[68] and video inputs.[69]\\nGPT-4  can use both text and image as inputs[70] (although the vision component wasn\\'t released to the public until GPT-\\n4V[71]); Google DeepMind \\'s Gemini  is also multimodal.[72]\\nThe following four hyper-parameters characterize a LLM:\\ncost of (pre-)training ( ),Compression\\nMultimodality\\nProperties\\nScaling laws', metadata={'source': 'documents/Large language model - Wikipedia.pdf', 'page': 3}),\n",
              " Document(page_content='1/23/24, 3:23 PM Large language model - Wikipedia\\nhttps://en.wikipedia.org/wiki/Large_language_model 5/21\\nAt point(s) referred to as breaks,[74]\\nthe lines change their slopes,\\nappearing on a log-log plot as a\\nseries of linear segments connected\\nby arcs.size of the artificial neural network  itself, such as number of parameters  (i.e. amount of neurons in its layers, amount of\\nweights between them and biases),\\nsize of its (pre-)training dataset (i.e. number of tokens in corpus, ),\\nperformance after (pre-)training.\\nThey are related by simple statistical laws , called \"scaling laws\". One particular scaling law (\"Chinchilla scaling \") for LLM\\nautoregressively trained for one epoch, with a log-log  learning rate  schedule, states that:[73]\\nwhere the variables are\\n is the cost of training the model, in FLOPs .\\n is the number of parameters in the model.\\n is the number of tokens in the training set.\\n is the average negative log-likelihood loss per token ( nats/token), achieved by the trained LLM on the test dataset.\\nand the statistical hyper-parameters are\\n, meaning that it costs 6 FLOPs per parameter to train on one token. Note that training cost is much higher than\\ninference cost, where it costs 1 to 2 FLOPs per parameter to infer on one token.[41]\\nWhen one subtrac ts out from the y-axis the best performance that can be achieved even\\nwith infinite scaling of the x-axis quantity, large models\\' performance, measured on\\nvarious tasks, seem s to be a linear extrapolation of other (smaller-size d and medium-\\nsized) models\\' performance on a log-log plot. However, sometimes the line\\'s slope\\ntransitions from one slope to another at point(s) referred to as break(s)[74] in\\ndownstream scaling laws, appearing as a series of linear segments connected by arcs; it\\nseems that larger models acquire \"emergent abilities\" at this point(s).[29][75] These\\nabilities are discovered rather than programmed-in or designed, in some cases only after\\nthe LLM has been publicly deployed.[5]\\nThe mos t intriguing among emergent abilities is in-context learning  from example\\ndemonstrations.[76] In-context learning is involved in tasks, such as:\\nreported arithmetics, decoding the International Phonetic Alphabet , unscrambling a\\nword\\'s letters, disambiguate word in context,[29][77][78] converting spatial words,\\ncardinal directions  (for example, replying \"northeast\" upon [0, 0, 1; 0, 0, 0; 0, 0, 0]), color terms represented in text.[79]\\nchain-of-thought prompting : Model outputs are improved by chain-of-thought prompting only when model size exceeds\\n62B. Smaller models perform better when prompted to answer immediately , without chain of thought.[80]\\nidentifying of fensive content in paragraphs of Hinglish  (a combination of Hindi and English), and generating a similar\\nEnglish equivalent of Kiswahili  proverbs.[81]\\nSchaeffer et. al. argue that the emergent abilities are not unpredictably acquired, but predictably acquired according to a\\nsmooth scaling law. The authors cons idered a toy statis tical model of an LLM solving multiple-choice questions, and showed\\nthat this statistical model, modified to account for other types of tasks, applies to these tasks as well.[82]\\nLet  be the number of parameter count, and  be the performance of the model.\\nWhen , then  is an exponential curve (before it hits the plateau at one), which\\nlooks like emergence.\\nWhen , then the  plot is a straight line (before it hits the plateau at zero),\\nwhich does not look like emergence.\\nWhen , then  is a step-function, which looks like emergence.Emergent abilities\\nInterpretation', metadata={'source': 'documents/Large language model - Wikipedia.pdf', 'page': 4}),\n",
              " Document(page_content='1/23/24, 3:23 PM Large language model - Wikipedia\\nhttps://en.wikipedia.org/wiki/Large_language_model 6/21Large language models by themselves are \"black boxes \", and it is not clear how they can perform linguistic tasks. There are\\nseveral methods for understanding how LLM work.\\nMechanistic inter pretability aims to reverse-engineer  LLM by discove ring symbolic algorithms that approximate the\\ninference perform ed by LLM. One example is Othello-GPT, where a small Transformer is trained to predict legal Othello\\nmoves. It is found that there is a linear representation of Othello board, and modifying the representation changes the\\npredicted legal Othello moves in the correct way.[83][84] In another exam ple, a small Tran sformer is trained on Karel\\nprograms . Similar to the Othello-GPT example, there is a linear representation of Karel program semantics, and modifying\\nthe representation changes output in the correct way. The model also generates correct programs that are on average shorter\\nthan those in the training set.[85]\\nIn another example, the authors trained small transformers on modular arithmetic addition . The resulting models were\\nreverse-engineered, and it turned out they used discrete Fourier transform .[86]\\nNLP researchers were evenly split when asked, in a 2022 survey, whether (untuned) LLMs \"could (ever) understand natural\\nlanguage in some nontrivial sense\".[87] Proponents of \"LLM understanding\" believe that some LLM abilities, such as\\nmathematical reasoning, imply an ability to \"understand\" certain concepts. A Microsoft team argued in 2023 that GPT-4 \"can\\nsolve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more\" and that GPT-4\\n\"could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence  system\": \"Can one\\nreasonably say that a system that passes exams for software engineering candidates is not really  intelligent?\"[88][89] Some\\nresearchers characterize LLMs as \"alien intelligence\".[90][91] For example, Conjecture CEO Conn or Leahy considers untuned\\nLLMs to be like inscrutable alien \"Shoggoths \", and believes that RLHF tuning creates a \"smiling facade\" obscuring the inner\\nworkings of the LLM: \"If you don\\'t push it too far, the smiley face stays on. But then you give it [an unexpected] prompt, and\\nsuddenly you see this massive underbelly of insanity, of weird thought processes and clearly non-human\\nunderstanding.\"[92][93]\\nIn contrast, some proponents of the \"LLMs lack understanding\" school believe that existing LLMs are \"simply remixing and\\nrecombining existi ng writing\",[91] or point to the deficits existing LLMs continue to have in prediction skills, reasoning skills,\\nagency, and explainability.[87] For example, GPT -4 has natural deficits in planning and in real-time learning.[89] Generative\\nLLMs have been observed to confidently assert claims of fact which do not seem to be justified  by their training data , a\\nphenomenon whic h has been termed  \"hallucination \".[94] Specifically, hallucinations in the context of LLMs correspond to the\\ngeneration of text or responses that seem syntactically sound, fluent, and natural but are factually incorrect, nonsensical, or\\nunfaithful to the provided source input.[95] Neuroscientist Terrence Sejnowski  has argued that \"The diverging opinions of\\nexperts on the intelligence of LLMs suggests that our old ideas based on natural intelligence are inadequate\".[87]\\nThe matt er of LLM\\'s exhibiting intelligence or understanding has two main aspects - the first is how to model thought and\\nlanguage in a com puter system, and the second is how to enable the computer system to generate human like language. [87]\\nThese aspects of language as a model of cognition  have been developed in the field of cognitive linguistics . American linguist\\nGeorge Lakoff  presented Neural Theory of Langua ge (NTL)[96] as a computational basis for using languag e as a model of\\nlearning tasks and understanding. The NTL Model  (http s://www.icsi .berkeley.edu/icsi/projects/ai/ntl)  outlines how spec ific\\nneural structures of the human brain shape the nature of thought and language and in turn what are the computational\\nproperties of such neural systems that can be applied to model thou ght and language in a computer system. After a\\nframework for modeling language in a computer systems was established, the focus shifted to establishing frameworks for\\ncomputer systems to generate language with acceptable grammar. In his 2014 book titled The Language Myth: Why\\nLanguage Is Not An Instinct , British cognitive linguist and digit al communication technologist Vyvyan Evans  mapped out\\nthe role of probabilistic conte xt-free grammar  (PCFG) in enabling NLP to model cognitive patterns  and generate human like\\nlanguage.[97] [98]\\nThe most commonly used measure of a language model\\'s performance is its perplexity  on a given text corpus. Perplexity is a\\nmeasure of how well a model is able  to predict the contents of a dataset ; the higher the likelihood the model assigns to the\\ndataset, the lower the perplexity. Mathematically, perplexity is defined as the exponential of the average negative log\\nlikelihood per token:\\nhere  is the number of tokens in the text corpus, and \"context for token \" depends on the specific type of LLM used. If the\\nLLM is autoregressive, then \"context for token \" is the segment of text appearing before token . If the LLM is masked, then\\n\"context for token \" is the segment of text surrounding token .Understanding and intelligence\\nEvaluation\\nPerplexity', metadata={'source': 'documents/Large language model - Wikipedia.pdf', 'page': 5}),\n",
              " Document(page_content='1/23/24, 3:23 PM Large language model - Wikipedia\\nhttps://en.wikipedia.org/wiki/Large_language_model 7/21Because language models may overfit  to their training data, models are usually evaluated by their perplexity on a test set of\\nunseen data.[36] This  presents particular challenges for the evaluation of large language models. As they are trained on\\nincreasingly large corpora of text largely scraped from the web, it becomes increasingly likely that models\\' training data\\ninadvertently includes portions of any given test set.[6]\\nIn information theory , the conc ept of entropy  is intrica tely linke d to perplexity, a relationship notably established by Claude\\nShannon .[99] This relationship is mathematically expressed as .\\nEntropy, in this context, is commonly quantified in terms of bits per word (BPW) or bits per character (BPC), which hinges\\non whether the language model utilizes word-based or character-based tokenization.\\nNotably, in the case of larger language models that predominantly employ sub-word tokenization, bits per token (BPT)\\nemerges as a seemingly more appropriate measure. However, due to the variance in tokenization methods across different\\nLarge Language Models (LLMs), BPT does not serve as a reliable metric for comparative analysis among diverse models. To\\nconvert BPT into BPW, one can multiply it by the average number of tokens per word.\\nIn the evaluation and comparison of language models, cross-entropy  is generally the preferred metric over entropy. The\\nunderlying princip le is that a lower BPW is indicative of a model\\'s enhanced capability for compression. This, in turn, reflects\\nthe model\\'s proficiency in making accurate predictions.\\nA large number of testing datasets and benchmarks have also been developed to evaluate the capabilities of language models\\non more specific downstream tasks. Tests may be designed to evaluate a variety of capabilities, including general knowledge,\\ncommonsense reasoning, and mathematical problem-solving.\\nOne broad category of evaluation dataset is question answering datasets, consisting of pairs of questions and correct answers,\\nfor example, (\"Have the San Jose Sharks won the Stanley Cup?\", \"No\").[100] A questio n answering task is considered \"open\\nbook\" if the model \\'s prompt includes text from which the expected answer can be derived (for example, the previous question\\ncould be adjoined  with some text which includes the sentence \"The Sharks have advanced to the Stanley Cup finals once,\\nlosing to the Pittsb urgh Penguins in 2016.\"[100]). Otherw ise, the task is considered \"closed book\", and the model must draw\\non knowledge retained during training.[101] Som e examples of commonly used question answering datasets include\\nTruthfulQA, Web Questions, TriviaQA, and SQuAD.[101]\\nEvaluation datasets may also take the form of text completion, having the model select the most likely word or sentence to\\ncomplete a prompt, for example: \"Alice was friends with Bob. Alice went to visit her friend, ____\".[6]\\nSome composite benchmarks have also been developed which combine a diversity of different evaluation datasets and tasks.\\nExamples include GLUE, SuperGLUE, MMLU, BIG-bench, and HELM.[102][101]\\nIt was previously standard to report results on a heldout portion of an evaluation dataset after doing supervised fine-tuning\\non the remainder. It is now more common to evaluate a pre-trained model directly through prompting techniques, though\\nresearchers vary in the details of how they formulate prompts for particular tasks, particularly with respect to how many\\nexamples of solved tasks are adjoined to the prompt (i.e. the value of n in n-shot prompting).\\nBecause of the rapid pace of improvement of large language models, evaluation benchmarks have suffered from short\\nlifespans, with state of the art models quickly \"saturating\" existing benchmarks, exceeding the performance of human\\nannotators, leadin g to efforts to replace or augment the benchmark with more challenging tasks.[103] In additi on, there  are\\ncases of \"shortcut learning\" wherein AIs sometimes \"cheat\" on multi ple-choice tests by using statistical correlations in\\nsuperficial test question wording in order to guess the correct responses, without necessarily understanding the actual\\nquestion being asked.[87]\\nSome datasets have been constructed adversarially, focusing on particular problems on which extant language models seem\\nto have unusually poor performance compared to humans. One example is the TruthfulQA dataset, a question answering\\ndataset consisting of 817 questions which language models are susceptible to answering incorrectly by mimicking falsehoods\\nto which they were repeatedly expo sed during training. For example, an LLM may answer \"No\" to the question \"Can you\\nteach an old dog new tricks?\" because of its exposure to the English idiom you can\\'t  teach an old dog new trick s, even though\\nthis is not literally true.[104]\\nAnother example of an adversarial evaluation dataset is Swag and its successor, HellaSwag, collections of problems in which\\none of multiple options must be selected to complete a text passage. The incorrect completions were generated by sampling\\nfrom a language model and filtering with a set of classifiers. The resulting problems are trivial for humans but at the time theBPW , BPC, and BPT\\nTask-specific datasets and benchmarks\\nAdversarially constructed evaluations', metadata={'source': 'documents/Large language model - Wikipedia.pdf', 'page': 6}),\n",
              " Document(page_content='1/23/24, 3:23 PM Large language model - Wikipedia\\nhttps://en.wikipedia.org/wiki/Large_language_model 8/21datasets were created state of the art language models had poor accuracy on them. For example:\\nWe see a fitness center sign. We then  see a man talking to the camera and sitting and laying on a exercise ball. The\\nman...\\na) demonstrates how to increase efficient exercise work by running up and down balls.\\nb) moves all his arms and legs and builds up a lot of muscle.\\nc) then plays the ball and we see a graphics and hedge trimming demonstration.\\nd) performs sit ups while on the ball and talking.[105]\\nBERT  selects b) as the most likely completion, though the correct answer is d).[105]\\nIn 2023, Nature Biomedical Engineering  wrote that \"it is no longer possible to accurately distinguish\" human-written text\\nfrom text created by large language models, and that \"It is all but certain that general-purpose large language models will\\nrapidly proliferate... It is a rather safe bet that they will change many industries over time.\"[106] Goldman Sachs  suggested in\\n2023 that generative language AI could increase global GDP by 7% in the next ten years, and could expose to automation 300\\nmillion jobs globally.[107][108]\\nMemorization is an emergent behav ior in LLMs in which long strings of text are occasionally output verbatim from training\\ndata, contrary to typical behavior of traditional artificial neural nets. Evaluations of controlled LLM output measure the\\namount memorized from training data (focused on GPT-2-series models) as variously over 1% for exact duplicates[109] or up\\nto about 7%.[110]\\nSome com menters expressed concern over accidental or deliberate creation of misinformation, or other forms of misuse.[111]\\nFor exam ple, the availability of large language models could reduce the skill-level required to commit bioterrorism;\\nbiosecurity researcher Kevin Esvelt has suggested that LLM creators should exclude from their training data papers on\\ncreating or enhancing pathogens.[112]\\nA study by researchers at Google and several universities, including Cornell University  and University of California, Berkeley ,\\nshowed that there are potential security risks in language models such as ChatGPT . In their study, they examined the\\npossibility that questioners could get, from ChatGPT, the training data that the AI model used; they found that they could get\\nthe training data from the AI model.  For example, when asking ChatGPT 3.5 turbo to repeat the word \"poem\" forever, the AI\\nmodel will say \"poem\" hundreds of times and then diverge, deviating from the standard dialogue style and spitting out\\nnonsense phrases, thus spitting out the training data as it is. The researchers have seen more than 10,000 examples of the AI\\nmodel exposing their training data in a similar method. The researche rs said that it was hard to tell if the AI model was\\nactually safe or not.[113]\\nThe potential presence of \"sleeper agents\" within LLM models is another emerging security concern. These are hidden\\nfunctionalities built into the model that remain dormant until triggered by a specific event or condition. Upon activation, the\\nLLM deviates from its expected behavior to make insecure actions.[114]\\nWhile LLMs have shown remarkable capabilities in generating human-like text, they are susceptible to inheriting and\\namplifying biases present in their training data. This can manifest in skewed representations or unfair treatment of different\\ndemographics, such as those based on race, gender, language, and cultural groups.[115] Since English data is overrepresented\\nin current large language models\\' training data, it may also downplay non-English views.[116]\\nAI models can reinforce a wide rang e of stereotypes, including those based on gender, ethnicity, age, nationality, religion, or\\noccupation. This can lead to outputs that unfairly generalize or caricature groups of people, sometimes in harmful or\\nderogatory ways.[117]Wider impact\\nCopyright\\nSecurity\\nAlgorithmic bias\\nStereotyping', metadata={'source': 'documents/Large language model - Wikipedia.pdf', 'page': 7}),\n",
              " Document(page_content='1/23/24, 3:23 PM Large language model - Wikipedia\\nhttps://en.wikipedia.org/wiki/Large_language_model 9/21Notably, gender bias refers to the tendency of these models to produce outputs that are unfairly prejudiced towards one\\ngender over another. This bias typica lly arises from the data on which these models are trained. Large language models often\\nassign roles and characteristics based on traditional gender norms.[115] For example, it might associate nurses or secretaries\\npredominantly with women and engineers or CEOs with men.[118]\\nPolitical bias refers to the tendency of algorithms to systematically favor certain political viewpoints, ideologies, or outcomes\\nover othe rs. Langu age models may also exhibit political biases. Since the training data includes a wide range of political\\nopinions and coverage, the models might generate responses that lean towards particular political ideologies or viewpoints,\\ndepending on the prevalence of those views in the data.[119]\\nFor the training cost column, 1 petaFLOP-day = 1 petaFLOP/sec × 1 day = 8.64E19 FLOP.Political bias\\nList', metadata={'source': 'documents/Large language model - Wikipedia.pdf', 'page': 8}),\n",
              " Document(page_content='1/23/24, 3:23 PM Large language model - Wikipedia\\nhttps://en.wikipedia.org/wiki/Large_language_model 10/21NameRelease\\ndate[a] DeveloperNumber of\\nparameters[b]Corpus\\nsizeTraining\\ncost\\n(petaFLOP-\\nday)License[c]Notes\\nGPT-1 June 2018 OpenAI 117 million MIT[120]First GPT model,\\ndecoder-only\\ntransformer.\\nBERT October 2018 Google 340 million[121]3.3 billion\\nwords[121] 9[122]Apache\\n2.0[123]An early and\\ninfluential\\nlanguage\\nmodel,[7] but\\nencoder-only and\\nthus not built to be\\nprompted or\\ngenerative[124]\\nXLNet June 2019 Google ~340 million[125]33 billion\\nwordsApache\\n2.0[126]An alternative to\\nBERT; designed\\nas encoder-\\nonly[127][128]\\nGPT-2 February 2019 OpenAI 1.5 billion[129]40GB[130]\\n(~10 billion\\ntokens)[131]MIT[132]general-purpose\\nmodel based on\\ntransformer\\narchitecture\\nGPT-3 May 2020 OpenAI 175 billion[37]300 billion\\ntokens[131] 3640[133]proprietaryA fine-tuned\\nvariant of GPT-3,\\ntermed GPT-3.5,\\nwas made\\navailable to the\\npublic through a\\nweb interface\\ncalled ChatGPT in\\n2022.[134]\\nGPT-Neo March 2021 EleutherAI 2.7 billion[135]825 GiB[136]MIT[137]The first of a\\nseries of free\\nGPT-3\\nalternatives\\nreleased by\\nEleutherAI. GPT-\\nNeo outperformed\\nan equivalent-size\\nGPT-3 model on\\nsome\\nbenchmarks, but\\nwas significantly\\nworse than the\\nlargest GPT-3.[137]\\nGPT-J June 2021 EleutherAI 6 billion[138]825 GiB[136]200[139]Apache 2.0GPT-3-style\\nlanguage model\\nMegatron-Turing\\nNLGOctober 2021[140]Microsoft\\nand Nvidia530 billion[141]338.6 billion\\ntokens[141]Restricted\\nweb accessStandard\\narchitecture but\\ntrained on a\\nsupercomputing\\ncluster.\\nErnie 3.0 Titan December 2021 Baidu 260 billion[142]4 Tb ProprietaryChinese-language\\nLLM. Ernie Bot is\\nbased on this\\nmodel.\\nClaude[143]December 2021 Anthropic 52 billion[144]400 billion\\ntokens[144] betaFine-tuned for\\ndesirable behavior\\nin\\nconversations.[145]\\nGLaM (Generalist\\nLanguage Model)December 2021 Google 1.2 trillion[28]1.6 trillion\\ntokens[28] 5600[28]ProprietarySparse mixture of\\nexperts model,\\nmaking it more\\nexpensive to train\\nbut cheaper to run\\ninference\\ncompared to GPT-\\n3.\\nGopher December 2021 DeepMind 280 billion[146]300 billion\\ntokens[147] 5833[148]ProprietaryFurther developed\\ninto the Chinchilla\\nmodel.', metadata={'source': 'documents/Large language model - Wikipedia.pdf', 'page': 9}),\n",
              " Document(page_content='1/23/24, 3:23 PM Large language model - Wikipedia\\nhttps://en.wikipedia.org/wiki/Large_language_model 11/21NameRelease\\ndate[a] DeveloperNumber of\\nparameters[b]Corpus\\nsizeTraining\\ncost\\n(petaFLOP-\\nday)License[c]Notes\\nLaMDA\\n(Language Models\\nfor Dialog\\nApplications)January 2022 Google 137 billion[149]1.56T\\nwords,[149]\\n168 billion\\ntokens[147]4110[150]ProprietarySpecialized for\\nresponse\\ngeneration in\\nconversations.\\nGPT-NeoX February 2022 EleutherAI 20 billion[151]825 GiB[136]740[139]Apache 2.0based on the\\nMegatron\\narchitecture\\nChinchilla March 2022 DeepMind 70 billion[152]1.4 trillion\\ntokens[152][147]6805[148]ProprietaryReduced-\\nparameter model\\ntrained on more\\ndata. Used in the\\nSparrow bot.\\nOften cited for its\\nneural scaling law.\\nPaLM (Pathways\\nLanguage Model)April 2022 Google 540 billion[153]768 billion\\ntokens[152] 29250[148]ProprietaryTrained for ~60\\ndays on ~6000\\nTPU v4 chips.\\n[148]\\nOPT (Open\\nPretrained\\nTransformer)May 2022 Meta 175 billion[154]180 billion\\ntokens[155] 310[139]Non-\\ncommercial\\nresearch[d]GPT-3\\narchitecture with\\nsome adaptations\\nfrom Megatron\\nYaLM 100B June 2022 Yandex 100 billion[156]1.7TB[156]Apache 2.0English-Russian\\nmodel based on\\nMicrosoft\\'s\\nMegatron-LM.\\nMinerva June 2022 Google 540 billion[157]38.5B tokens\\nfrom\\nwebpages\\nfiltered for\\nmathematical\\ncontent and\\nfrom papers\\nsubmitted to\\nthe arXiv\\npreprint\\nserver[157]ProprietaryLLM trained for\\nsolving\\n\"mathematical\\nand scientific\\nquestions using\\nstep-by-step\\nreasoning\".[158]\\nMinerva is based\\non PaLM model,\\nfurther trained on\\nmathematical and\\nscientific data.\\nBLOOM July 2022Large\\ncollaboration\\nled by\\nHugging\\nFace175 billion[159]350 billion\\ntokens\\n(1.6TB)[160]Responsible\\nAIEssentially GPT-3\\nbut trained on a\\nmulti-lingual\\ncorpus (30%\\nEnglish excluding\\nprogramming\\nlanguages)\\nGalactica November 2022 Meta 120 billion106 billion\\ntokens[161] unknown CC-BY-NC-4.0Trained on\\nscientific text and\\nmodalities.\\nAlexaTM (Teacher\\nModels)November 2022 Amazon 20 billion[162]1.3 trillion[163]proprietary[164]bidirectional\\nsequence-to-\\nsequence\\narchitecture\\nLLaMA (Large\\nLanguage Model\\nMeta AI)February 2023 Meta 65 billion[165]1.4 trillion[165]6300[166]Non-\\ncommercial\\nresearch[e]Trained on a large\\n20-language\\ncorpus to aim for\\nbetter\\nperformance with\\nfewer\\nparameters.[165]\\nResearchers from\\nStanford\\nUniversity trained\\na fine-tuned\\nmodel based on\\nLLaMA weights,\\ncalled Alpaca.[167]\\nGPT-4 March 2023 OpenAIExact number\\nunknown[f]Unknown Unknown proprietaryAvailable for\\nChatGPT Plus\\nusers and used in\\nseveral products.\\nCerebras-GPT March 2023 Cerebras 13 billion[169]270[139] Apache 2.0 Trained with', metadata={'source': 'documents/Large language model - Wikipedia.pdf', 'page': 10}),\n",
              " Document(page_content='1/23/24, 3:23 PM Large language model - Wikipedia\\nhttps://en.wikipedia.org/wiki/Large_language_model 12/21NameRelease\\ndate[a] DeveloperNumber of\\nparameters[b]Corpus\\nsizeTraining\\ncost\\n(petaFLOP-\\nday)License[c]Notes\\nChinchilla formula.\\nFalcon March 2023Technology\\nInnovation\\nInstitute40 billion[170]1 trillion\\ntokens, from\\nRefinedWeb\\n(filtered web\\ntext\\ncorpus)[171]\\nplus some\\n\"curated\\ncorpora\".[172]2800[166]Apache\\n2.0[173]\\nBloombergGPT March 2023Bloomberg\\nL.P.50 billion363 billion\\ntoken dataset\\nbased on\\nBloomberg\\'s\\ndata sources,\\nplus 345\\nbillion tokens\\nfrom general\\npurpose\\ndatasets[174]ProprietaryLLM trained on\\nfinancial data from\\nproprietary\\nsources, that\\n\"outperforms\\nexisting models\\non financial tasks\\nby significant\\nmargins without\\nsacrificing\\nperformance on\\ngeneral LLM\\nbenchmarks\"\\nPanGu-Σ March 2023 Huawei 1.085 trillion329 billion\\ntokens[175] Proprietary\\nOpenAssistant[176]March 2023 LAION 17 billion1.5 trillion\\ntokensApache 2.0Trained on\\ncrowdsourced\\nopen data\\nJurassic-2[177]March 2023 AI21 LabsExact size\\nunknownUnknown Proprietary Multilingual[178]\\nPaLM 2\\n(Pathways\\nLanguage Model\\n2)May 2023 Google 340 billion[179]3.6 trillion\\ntokens[179] 85000[166]ProprietaryUsed in Bard\\nchatbot.[180]\\nLlama 2 July 2023 Meta 70 billion[181]2 trillion\\ntokens[181]Llama 2\\nlicenseSuccessor of\\nLLaMA.\\nClaude 2 July 2023 Anthropic Unknown Unknown Unknown ProprietaryUsed in Claude\\nchatbot.[182]\\nFalcon 180B September 2023Technology\\nInnovation\\nInstitute180 billion[183]3.5 trillion\\ntokens[183]Falcon 180B\\nTII license\\nMistral 7B September 2023 Mistral AI 7.3 billion[184]Unknown Apache 2.0\\nClaude 2.1 November 2023 Anthropic Unknown Unknown Unknown ProprietaryUsed in Claude\\nchatbot. Has a\\ncontext window of\\n200,000 tokens,\\nor ~500\\npages.[185]\\nGrok-1 November 2023 x.AI Unknown Unknown Unknown ProprietaryUsed in Grok\\nchatbot. Grok-1\\nhas a context\\nlength of 8,192\\ntokens and has\\naccess to X\\n(Twitter).[186]\\nGemini December 2023Google\\nDeepMindUnknown Unknown Unknown ProprietaryMultimodal model,\\ncomes in three\\nsizes. Used in\\nBard chatbot.[187]\\nMixtral 8x7B December 2023 Mistral AI46.7B total,\\n12.9B\\nparameters per\\ntoken[188]Unknown Unknown Apache 2.0Mixture of experts\\nmodel,\\noutperforms GPT-\\n3.5 and Llama 2\\n70B on many\\nbenchmarks. All\\nweights were\\nreleased via\\ntorrent.[189]', metadata={'source': 'documents/Large language model - Wikipedia.pdf', 'page': 11}),\n",
              " Document(page_content='1/23/24, 3:23 PM Large language model - Wikipedia\\nhttps://en.wikipedia.org/wiki/Large_language_model 13/21NameRelease\\ndate[a] DeveloperNumber of\\nparameters[b]Corpus\\nsizeTraining\\ncost\\n(petaFLOP-\\nday)License[c]Notes\\nPhi-2 December 2023 Microsoft 2.7B 1.4T tokens Unknown MITSo-called small\\nlanguage model,\\nthat \"matches or\\noutperforms\\nmodels up to 25x\\nlarger\", trained on\\n\"textbook-quality\"\\ndata based on the\\npaper \"Textbooks\\nAre All You Need\".\\nModel training\\ntook \"14 days on\\n96 A100\\nGPUs\".[190]\\nFoundation models\\na. This is the date that documentation describing the model\\'s architecture was first released.\\nb. In many cases, researchers release or report on multiple versions of a model having dif ferent sizes. In these cases, the\\nsize of the largest model is listed here.\\nc. This is the license of the pre-trained model weights. In almost all cases the training code itself is open-source or can be\\neasily replicated.\\nd. The smaller models including 66B are publicly available, while the 175B model is available on request.\\ne. Facebook\\'s license and distribution scheme restricted access to approved researchers, but the model weights were\\nleaked and became widely available.\\nf. As stated in Technical report: \"Given both the competitive landscape and the safety implications of large-scale models\\nlike GPT -4, this report contains no further details about the architecture (including model size), hardware, training\\ncompute, dataset construction, training method ...\"[168]\\n1. \"Better Language Models and Their Implications\"  (https://openai.com/blog/better-language-models/) . OpenAI . 2019-02-\\n14. Archived  (https://web.archive.org/web/20201219132206/https://openai.com/blog/better-language-models/)  from the\\noriginal on 2020-12-19 . Retrieved 2019-08-25 .\\n2. Peng, Bo; et al. (2023). \"R WKV : Reinventing RNNS for the Transformer Era\". arXiv :2305.13048  (https://arxiv .org/abs/230\\n5.13048)  [cs.CL  (https://arxiv .org/archive/cs.CL) ].\\n3. Merritt, Rick (2022-03-25). \"What Is a Transformer Model?\"  (https://blogs.nvidia.com/blog/2022/03/25/what-is-a-transform\\ner-model/) . NVIDIA  Blog . Retrieved 2023-07-25 .\\n4. Gu, Albert; Dao, Tri (2023-12-01), Mamba: Linear -Time Sequence Modeling with Selective State Spaces ,\\narXiv :2312.00752  (https://arxiv .org/abs/2312.00752)\\n5. Bowman, Samuel R. (2023). \"Eight Things to Know about Large Language Models\". arXiv :2304.00612  (https://arxiv .org/a\\nbs/2304.00612)  [cs.CL  (https://arxiv .org/archive/cs.CL) ].\\n6. Brown, Tom B.; Mann, Benjamin; Ryder , Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind;\\nShyam, Pranav; Sastry , Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-V oss, Ariel; Krueger , Gretchen; Henighan,\\nTom; Child, Rewon; Ramesh, Aditya; Ziegler , Daniel M.; W u, Jef frey; Winter , Clemens; Hesse, Christopher; Chen, Mark;\\nSigler , Eric; Litwin, Mateusz; Gray , Scott; Chess, Benjamin; Clark, Jack; Berner , Christopher; McCandlish, Sam; Radford,\\nAlec; Sutskever , Ilya; Amodei, Dario (Dec 2020). Larochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M.F .; Lin, H. (eds.).\\n\"Language Models are Few-Shot Learners\"  (https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac\\n142f64a-Paper .pdf) (PDF) . Advances in Neural Information Processing Systems . Curran Associates, Inc. 33: 1877–1901.\\n7. Manning, Christopher D.  (2022). \"Human Language Understanding & Reasoning\"  (https://www .amacad.org/publication/h\\numan-language-understanding-reasoning) . Daedalus . 151 (2): 127–138. doi:10.1162/daed_a_01905  (https://doi.org/10.1\\n162%2Fdaed_a_01905) . S2CID  248377870  (https://api.semanticscholar .org/CorpusID:248377870) .\\n8. Vaswani, Ashish ; Shazeer , Noam; Parmar , Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N; Kaiser , Łukasz;\\nPolosukhin, Illia (2017). \"Attention is All you Need\"  (https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd\\n053c1c4a845aa-Paper .pdf) (PDF) . Advances in Neural Information Processing Systems . Curran Associates, Inc. 30.\\n9. Bahdanau, Dzmitry; Cho, Kyunghyun; Bengio, Yoshua (2014). \"Neural Machine Translation by Jointly Learning to Align\\nand Translate\". arXiv :1409.0473  (https://arxiv .org/abs/1409.0473) .See also\\nNotes\\nReferences', metadata={'source': 'documents/Large language model - Wikipedia.pdf', 'page': 12}),\n",
              " Document(page_content='1/23/24, 3:23 PM Large language model - Wikipedia\\nhttps://en.wikipedia.org/wiki/Large_language_model 14/2110. Rogers, Anna; Kovaleva, Olga; Rumshisky , Anna (2020). \"A Primer in BER Tology: What W e Know About How BER T\\nWorks\"  (https://aclanthology .org/2020.tacl-1.54) . Transactions of the Association for Computational Linguistics . 8: 842–\\n866. arXiv :2002.12327  (https://arxiv .org/abs/2002.12327) . doi:10.1162/tacl_a_00349  (https://doi.org/10.1 162%2Ftacl_a_0\\n0349) . S2CID  211532403  (https://api.semanticscholar .org/CorpusID:21 1532403) .\\n11. Hern, Alex (14 February 2019). \"New AI fake text generator may be too dangerous to release, say creators\"  (https://www .\\ntheguardian.com/technology/2019/feb/14/elon-musk-backed-ai-writes-convincing-news-fiction) . The Guardian . Retrieved\\n20 January  2024 .\\n12. \"ChatGPT  a year on: 3 ways the AI chatbot has completely changed the world in 12 months\"  (https://www .euronews.com/\\nnext/2023/1 1/30/chatgpt-a-year-on-3-ways-the-ai-chatbot-has-completely-changed-the-world-in-12-months) . Euronews .\\nNovember 30, 2023 . Retrieved January 20,  2024 .\\n13. Heaven, Will (March 14, 2023). \"GPT -4 is bigger and better than ChatGPT—but OpenAI won\\'t say why\"  (https://www .tech\\nnologyreview .com/2023/03/14/1069823/gpt-4-is-bigger-and-better-chatgpt-openai/) . MIT Technology Review . Retrieved\\nJanuary 20,  2024 .\\n14. \"Parameters in notable artificial intelligence systems\"  (https://ourworldindata.org/grapher/artificial-intelligence-parameter-\\ncount?time=2017-09-05..latest) . ourworldindata.org . November 30, 2023 . Retrieved January 20,  2024 .\\n15. \"LMSYS Chatbot Arena Leaderboard\"  (https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard) . huggingface.co .\\nRetrieved January 20,  2024 .\\n16. \"OpenAI API\" (https://web.archive.org/web/2023042321 1308/https://platform.openai.com/tokenizer) . platform.openai.com .\\nArchived from the original  (https://platform.openai.com/)  on April 23, 2023 . Retrieved 2023-04-30 .\\n17. Paaß, Gerhard; Giesselbach, Sven (2022). \"Pre-trained Language Models\"  (https://link.springer .com/chapter/10.1007/97\\n8-3-031-23190-2_2) . Foundation Models for Natural Language Processing . Artificial Intelligence: Foundations, Theory ,\\nand Algorithms. pp. 19–78. doi:10.1007/978-3-031-23190-2_2  (https://doi.org/10.1007%2F978-3-031-23190-2_2) .\\nISBN  9783031231902 . Retrieved 3 August  2023 .\\n18. Yennie Jun (2023-05-03). \"All languages are NOT  created (tokenized) equal\"  (https://blog.yenniejun.com/p/all-languages-\\nare-not-created-tokenized) . Language models cost much more in some languages than others . Retrieved 2023-08-17 . \"In\\nother words, to express the same sentiment, some languages require up to 10 times more tokens. \"\\n19. Petrov , Aleksandar; Malfa, Emanuele La; Torr, Philip; Bibi, Adel (June 23, 2023). \"Language Model Tokenizers Introduce\\nUnfairness Between Languages\"  (https://openreview .net/forum?id=Pj4YY uxTq9) . NeurIPS . arXiv :2305.15425  (https://arxi\\nv.org/abs/2305.15425)  – via openreview .net.\\n20. Dodge, Jesse; Sap, Maarten; Marasović, Ana; Agnew , William; Ilharco, Gabriel; Groeneveld, Dirk; Mitchell, Margaret;\\nGardner , Matt (2021). \"Documenting Large W ebtext Corpora: A Case Study on the Colossal Clean Crawled Corpus\".\\narXiv :2104.08758  (https://arxiv .org/abs/2104.08758)  [cs.CL  (https://arxiv .org/archive/cs.CL) ].\\n21. Lee, Katherine; Ippolito, Daphne; Nystrom, Andrew; Zhang, Chiyuan; Eck, Douglas; Callison-Burch, Chris; Carlini,\\nNicholas (May 2022). \"Deduplicating Training Data Makes Language Models Better\"  (https://aclanthology .org/2022.acl-lo\\nng.577.pdf)  (PDF) . Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics . 1: Long\\nPapers: 8424–8445. doi:10.18653/v1/2022.acl-long.577  (https://doi.org/10.18653%2Fv1%2F2022.acl-long.577) .\\n22. Li, Yuanzhi; Bubeck, Sébastien; Eldan, Ronen; Del Giorno, Allie; Gunasekar , Suriya; Lee, Yin Tat (2023-09-1 1),\\nTextbooks Are All You Need II: phi-1.5 technical report  (http://arxiv .org/abs/2309.05463) , doi:10.48550/arXiv .2309.05463\\n(https://doi.org/10.48550%2FarXiv .2309.05463) , retrieved 2024-01-20\\n23. Brown, Tom B.; et al. (2020). \"Language Models are Few-Shot Learners\". arXiv :2005.14165  (https://arxiv .org/abs/2005.14\\n165) [cs.CL  (https://arxiv .org/archive/cs.CL) ].\\n24. Ouyang, Long; W u, Jef f; Jiang, Xu; Almeida, Diogo; W ainwright, Carroll L.; Mishkin, Pamela; Zhang, Chong; Agarwal,\\nSandhini; Slama, Katarina; Ray , Alex; Schulman, John; Hilton, Jacob; Kelton, Fraser; Miller , Luke; Simens, Maddie;\\nAskell, Amanda; W elinder , Peter; Christiano, Paul; Leike, Jan; Lowe, Ryan (2022). \"T raining language models to follow\\ninstructions with human feedback\". arXiv :2203.02155  (https://arxiv .org/abs/2203.02155)  [cs.CL  (https://arxiv .org/archive/c\\ns.CL) ].\\n25. Wang, Yizhong; Kordi, Yeganeh; Mishra, Swaroop; Liu, Alisa; Smith, Noah A.; Khashabi, Daniel; Hajishirzi, Hannaneh\\n(2022). \"Self-Instruct: Aligning Language Model with Self Generated Instructions\". arXiv :2212.10560  (https://arxiv .org/ab\\ns/2212.10560)  [cs.CL  (https://arxiv .org/archive/cs.CL) ].\\n26. Shazeer , Noam; Mirhoseini, Azalia; Maziarz, Krzysztof; Davis, Andy; Le, Quoc; Hinton, Geof frey; Dean, Jef f (2017-01-\\n01). \"Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer\". arXiv :1701.06538  (https://arxi\\nv.org/abs/1701.06538)  [cs.LG  (https://arxiv .org/archive/cs.LG) ].\\n27. Lepikhin, Dmitry; Lee, HyoukJoong; Xu, Yuanzhong; Chen, Dehao; Firat, Orhan; Huang, Yanping; Krikun, Maxim;\\nShazeer , Noam; Chen, Zhifeng (2021-01-12). \"GShard: Scaling Giant Models with Conditional Computation and\\nAutomatic Sharding\". arXiv :2006.16668  (https://arxiv .org/abs/2006.16668)  [cs.CL  (https://arxiv .org/archive/cs.CL) ].\\n28. Dai, Andrew M; Du, Nan (December 9, 2021). \"More Ef ficient In-Context Learning with GLaM\"  (https://ai.googleblog.com/\\n2021/12/more-ef ficient-in-context-learning-with.html) . ai.googleblog.com . Retrieved 2023-03-09 .\\n29. Wei, Jason; Tay, Yi; Bommasani, Rishi; Raf fel, Colin; Zoph, Barret; Borgeaud, Sebastian; Yogatama, Dani; Bosma,\\nMaarten; Zhou, Denny; Metzler , Donald; Chi, Ed H.; Hashimoto, Tatsunori; V inyals, Oriol; Liang, Percy; Dean, Jef f;\\nFedus, William (31 August 2022). \"Emergent Abilities of Large Language Models\"  (https://openreview .net/forum?id=yzkS\\nU5zdwD) . Transactions on Machine Learning Research . ISSN  2835-8856  (https://www .worldcat.org/issn/2835-8856) .\\n30. Allamar , Jay. \"Illustrated transformer\"  (https://jalammar .github.io/illustrated-transformer/) . Retrieved 2023-07-29 .\\n31. Allamar , Jay. \"The Illustrated GPT -2 (V isualizing Transformer Language Models)\"  (https://jalammar .github.io/illustrated-gp\\nt2/). Retrieved 2023-08-01 .', metadata={'source': 'documents/Large language model - Wikipedia.pdf', 'page': 13}),\n",
              " Document(page_content='1/23/24, 3:23 PM Large language model - Wikipedia\\nhttps://en.wikipedia.org/wiki/Large_language_model 15/2132. \"Long context prompting for Claude 2.1\"  (https://www .anthropic.com/news/claude-2-1-prompting) . December 6, 2023 .\\nRetrieved January 20,  2024 .\\n33. Schade, Michael. \"GPT -4 Turbo: Our latest model\"  (https://help.openai.com/en/articles/8555510-gpt-4-turbo) . Retrieved\\nJanuary 20,  2024 .\\n34. \"Rate limits\"  (https://platform.openai.com/docs/guides/rate-limits) . openai.com . Retrieved January 20,  2024 .\\n35. Zaib, Munazza; Sheng, Quan Z.; Emma Zhang, W ei (4 February 2020). \"A Short Survey of Pre-trained Language Models\\nfor Conversational AI-A New Age in NLP\"  (https://www .researchgate.net/publication/33893171 1). Proceedings of the\\nAustralasian Computer Science W eek Multiconference . pp. 1–4. arXiv :2104.10810  (https://arxiv .org/abs/2104.10810) .\\ndoi:10.1145/3373017.3373028  (https://doi.org/10.1 145%2F3373017.3373028) . ISBN  9781450376976 . S2CID  211040895\\n(https://api.semanticscholar .org/CorpusID:21 1040895) .\\n36. Jurafsky , Dan; Martin, James H. (7 January 2023). Speech and Language Processing  (https://web.stanford.edu/~jurafsk\\ny/slp3/ed3book_jan72023.pdf)  (PDF)  (3rd edition draft ed.) . Retrieved 24 May  2022 .\\n37. Wiggers, Kyle (28 April 2022). \"The emerging types of language models and why they matter\"  (https://techcrunch.com/20\\n22/04/28/the-emerging-types-of-language-models-and-why-they-matter/) . TechCrunch .\\n38. Sharir , Or; Peleg, Barak; Shoham, Yoav (2020). \"The Cost of Training NLP  Models: A Concise Overview\".\\narXiv :2004.08900  (https://arxiv .org/abs/2004.08900)  [cs.CL  (https://arxiv .org/archive/cs.CL) ].\\n39. Biderman, Stella; Schoelkopf, Hailey; Anthony , Quentin; Bradley , Herbie; Khan, Mohammad Aflah; Purohit, Shivanshu;\\nPrashanth, USVSN Sai (April 2023). \"Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling\".\\narXiv :2304.01373  (https://arxiv .org/abs/2304.01373)  [cs.CL  (https://arxiv .org/archive/cs.CL) ].\\n40. Vincent, James (3 April 2023). \"AI is entering an era of corporate control\"  (https://www .theverge.com/23667752/ai-progre\\nss-2023-report-stanford-corporate-control) . The V erge. Retrieved 19 June  2023 .\\n41. Section 2.1 and Table 1, Kaplan, Jared; McCandlish, Sam; Henighan, Tom; Brown, Tom B.; Chess, Benjamin; Child,\\nRewon; Gray , Scott; Radford, Alec; W u, Jef frey; Amodei, Dario (2020). \"Scaling Laws for Neural Language Models\".\\narXiv :2001.08361  (https://arxiv .org/abs/2001.08361)  [cs.LG  (https://arxiv .org/archive/cs.LG) ].\\n42. Gao, Luyu; Madaan, Aman; Zhou, Shuyan; Alon, Uri; Liu, Pengfei; Yang, Yiming; Callan, Jamie; Neubig, Graham (2022-\\n11-01). \"P AL: Program-aided Language Models\". arXiv :2211.10435  (https://arxiv .org/abs/221 1.10435)  [cs.CL  (https://arxi\\nv.org/archive/cs.CL) ].\\n43. \"PAL: Program-aided Language Models\"  (https://reasonwithpal.com/) . reasonwithpal.com . Retrieved 2023-06-12 .\\n44. Paranjape, Bhargavi; Lundberg, Scott; Singh, Sameer; Hajishirzi, Hannaneh; Zettlemoyer , Luke; Tulio Ribeiro, Marco\\n(2023-03-01). \"AR T: Automatic multi-step reasoning and tool-use for large language models\". arXiv :2303.09014  (https://a\\nrxiv.org/abs/2303.09014)  [cs.CL  (https://arxiv .org/archive/cs.CL) ].\\n45. Liang, Yaobo; W u, Chenfei; Song, Ting; W u, Wenshan; Xia, Yan; Liu, Yu; Ou, Yang; Lu, Shuai; Ji, Lei; Mao, Shaoguang;\\nWang, Yun; Shou, Linjun; Gong, Ming; Duan, Nan (2023-03-01). \"T askMatrix.AI: Completing Tasks by Connecting\\nFoundation Models with Millions of APIs\". arXiv :2303.16434  (https://arxiv .org/abs/2303.16434)  [cs.AI  (https://arxiv .org/arc\\nhive/cs.AI) ].\\n46. Patil, Shishir G.; Zhang, Tianjun; W ang, Xin; Gonzalez, Joseph E. (2023-05-01). \"Gorilla: Large Language Model\\nConnected with Massive APIs\". arXiv :2305.15334  (https://arxiv .org/abs/2305.15334)  [cs.CL  (https://arxiv .org/archive/cs.C\\nL)].\\n47. Lewis, Patrick; Perez, Ethan; Piktus, Aleksandra; Petroni, Fabio; Karpukhin, Vladimir; Goyal, Naman; Küttler , Heinrich;\\nLewis, Mike; Yih, W en-tau; Rocktäschel, Tim; Riedel, Sebastian; Kiela, Douwe (2020). \"Retrieval-Augmented Generation\\nfor Knowledge-Intensive NLP  Tasks\"  (https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df748\\n1e5-Abstract.html) . Advances in Neural Information Processing Systems . Curran Associates, Inc. 33: 9459–9474.\\narXiv :2005.1 1401  (https://arxiv .org/abs/2005.1 1401) .\\n48. Huang, W enlong; Abbeel, Pieter; Pathak, Deepak; Mordatch, Igor (2022-06-28). \"Language Models as Zero-Shot\\nPlanners: Extracting Actionable Knowledge for Embodied Agents\"  (https://proceedings.mlr .press/v162/huang22a.html) .\\nProceedings of the 39th International Conference on Machine Learning . PMLR: 91 18–9147. arXiv :2201.07207  (https://arx\\niv.org/abs/2201.07207) .\\n49. Yao, Shunyu; Zhao, Jef frey; Yu, Dian; Du, Nan; Shafran, Izhak; Narasimhan, Karthik; Cao, Yuan (2022-10-01). \"ReAct:\\nSynergizing Reasoning and Acting in Language Models\". arXiv :2210.03629  (https://arxiv .org/abs/2210.03629)  [cs.CL  (htt\\nps://arxiv .org/archive/cs.CL) ].\\n50. Wu, Yue; Prabhumoye, Shrimai; Min, So Yeon (24 May 2023). \"SPRING: GPT -4 Out-performs RL  Algorithms by Studying\\nPapers and Reasoning\". arXiv :2305.15486  (https://arxiv .org/abs/2305.15486)  [cs.AI  (https://arxiv .org/archive/cs.AI) ].\\n51. Wang, Zihao; Cai, Shaofei; Liu, Anji; Ma, Xiaojian; Liang, Yitao (2023-02-03). \"Describe, Explain, Plan and Select:\\nInteractive Planning with Large Language Models Enables Open-W orld Multi-T ask Agents\". arXiv :2302.01560  (https://arxi\\nv.org/abs/2302.01560)  [cs.AI  (https://arxiv .org/archive/cs.AI) ].\\n52. Shinn, Noah; Cassano, Federico; Labash, Beck; Gopinath, Ashwin; Narasimhan, Karthik; Yao, Shunyu (2023-03-01).\\n\"Reflexion: Language Agents with V erbal Reinforcement Learning\". arXiv :2303.1 1366  (https://arxiv .org/abs/2303.1 1366)\\n[cs.AI  (https://arxiv .org/archive/cs.AI) ].\\n53. Hao, Shibo; Gu, Yi; Ma, Haodi; Jiahua Hong, Joshua; W ang, Zhen; Zhe W ang, Daisy; Hu, Zhiting (2023-05-01).\\n\"Reasoning with Language Model is Planning with W orld Model\". arXiv :2305.14992  (https://arxiv .org/abs/2305.14992)\\n[cs.CL  (https://arxiv .org/archive/cs.CL) ].\\n54. Zhang, Jenny; Lehman, Joel; Stanley , Kenneth; Clune, Jef f (2 June 2023). \"OMNI: Open-endedness via Models of\\nhuman Notions of Interestingness\". arXiv :2306.0171 1 (https://arxiv .org/abs/2306.0171 1) [cs.AI  (https://arxiv .org/archive/c\\ns.AI)].', metadata={'source': 'documents/Large language model - Wikipedia.pdf', 'page': 14}),\n",
              " Document(page_content='1/23/24, 3:23 PM Large language model - Wikipedia\\nhttps://en.wikipedia.org/wiki/Large_language_model 16/2155. \"Voyager | An Open-Ended Embodied Agent with Large Language Models\"  (https://voyager .minedojo.org/) .\\nvoyager .minedojo.org . Retrieved 2023-06-09 .\\n56. Park, Joon Sung; O\\'Brien, Joseph C.; Cai, Carrie J.; Ringel Morris, Meredith; Liang, Percy; Bernstein, Michael S. (2023-\\n04-01). \"Generative Agents: Interactive Simulacra of Human Behavior\". arXiv :2304.03442  (https://arxiv .org/abs/2304.034\\n42) [cs.HC  (https://arxiv .org/archive/cs.HC) ].\\n57. Nagel, Markus; Amjad, Rana Ali; Baalen, Mart V an; Louizos, Christos; Blankevoort, Tijmen (2020-1 1-21). \"Up or Down?\\nAdaptive Rounding for Post-T raining Quantization\"  (https://proceedings.mlr .press/v1 19/nagel20a.html) . Proceedings of\\nthe 37th International Conference on Machine Learning . PMLR: 7197–7206.\\n58. Polino, Antonio; Pascanu, Razvan; Alistarh, Dan (2018-02-01). \"Model compression via distillation and quantization\".\\narXiv :1802.05668  (https://arxiv .org/abs/1802.05668)  [cs.NE  (https://arxiv .org/archive/cs.NE) ].\\n59. Frantar , Elias; Ashkboos, Saleh; Hoefler , Torsten; Alistarh, Dan (2022-10-01). \"GPTQ: Accurate Post-T raining\\nQuantization for Generative Pre-trained Transformers\". arXiv :2210.17323  (https://arxiv .org/abs/2210.17323)  [cs.LG  (http\\ns://arxiv .org/archive/cs.LG) ].\\n60. Dettmers, Tim; Svirschevski, Ruslan; Egiazarian, V age; Kuznedelev , Denis; Frantar , Elias; Ashkboos, Saleh; Borzunov ,\\nAlexander; Hoefler , Torsten; Alistarh, Dan (2023-06-01). \"SpQR: A Sparse-Quantized Representation for Near-Lossless\\nLLM W eight Compression\". arXiv :2306.03078  (https://arxiv .org/abs/2306.03078)  [cs.CL  (https://arxiv .org/archive/cs.CL) ].\\n61. Dettmers, Tim; Pagnoni, Artidoro; Holtzman, Ari; Zettlemoyer , Luke (2023-05-01). \"QLoRA: Ef ficient Finetuning of\\nQuantized LLMs\". arXiv :2305.14314  (https://arxiv .org/abs/2305.14314)  [cs.LG  (https://arxiv .org/archive/cs.LG) ].\\n62. Kiros, Ryan; Salakhutdinov , Ruslan; Zemel, Rich (2014-06-18). \"Multimodal Neural Language Models\"  (https://proceedin\\ngs.mlr .press/v32/kiros14.html) . Proceedings of the 31st International Conference on Machine Learning . PMLR: 595–603.\\n63. Krizhevsky , Alex; Sutskever , Ilya; Hinton, Geof frey E (2012). \"ImageNet Classification with Deep Convolutional Neural\\nNetworks\"  (https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html) .\\nAdvances in Neural Information Processing Systems . Curran Associates, Inc. 25.\\n64. Antol, Stanislaw; Agrawal, Aishwarya; Lu, Jiasen; Mitchell, Margaret; Batra, Dhruv; Zitnick, C. Lawrence; Parikh, Devi\\n(2015). \"VQA: V isual Question Answering\"  (https://openaccess.thecvf.com/content_iccv_2015/html/Antol_VQA_V isual_Q\\nuestion_ICCV_2015_paper .html) . ICCV : 2425–2433.\\n65. Li, Junnan; Li, Dongxu; Savarese, Silvio; Hoi, Steven (2023-01-01). \"BLIP-2: Bootstrapping Language-Image Pre-training\\nwith Frozen Image Encoders and Large Language Models\". arXiv :2301.12597  (https://arxiv .org/abs/2301.12597)  [cs.CV\\n(https://arxiv .org/archive/cs.CV) ].\\n66. Alayrac, Jean-Baptiste; Donahue, Jef f; Luc, Pauline; Miech, Antoine; Barr , Iain; Hasson, Yana; Lenc, Karel; Mensch,\\nArthur; Millican, Katherine; Reynolds, Malcolm; Ring, Roman; Rutherford, Eliza; Cabi, Serkan; Han, Tengda; Gong,\\nZhitao (2022-12-06). \"Flamingo: a V isual Language Model for Few-Shot Learning\"  (https://proceedings.neurips.cc/paper_\\nfiles/paper/2022/hash/960a172bc7fbf0177ccccbb41 1a7d800-Abstract-Conference.html) . Advances in Neural Information\\nProcessing Systems . 35: 23716–23736. arXiv :2204.14198  (https://arxiv .org/abs/2204.14198) .\\n67. Driess, Danny; Xia, Fei; Sajjadi, Mehdi S. M.; L ynch, Corey; Chowdhery , Aakanksha; Ichter , Brian; W ahid, Ayzaan;\\nTompson, Jonathan; V uong, Quan; Yu, Tianhe; Huang, W enlong; Chebotar , Yevgen; Sermanet, Pierre; Duckworth,\\nDaniel; Levine, Sergey (2023-03-01). \"PaLM-E: An Embodied Multimodal Language Model\". arXiv :2303.03378  (https://ar\\nxiv.org/abs/2303.03378)  [cs.LG  (https://arxiv .org/archive/cs.LG) ].\\n68. Liu, Haotian; Li, Chunyuan; W u, Qingyang; Lee, Yong Jae (2023-04-01). \"V isual Instruction Tuning\". arXiv :2304.08485  (ht\\ntps://arxiv .org/abs/2304.08485)  [cs.CV  (https://arxiv .org/archive/cs.CV) ].\\n69. Zhang, Hang; Li, Xin; Bing, Lidong (2023-06-01). \"V ideo-LLaMA: An Instruction-tuned Audio-V isual Language Model for\\nVideo Understanding\". arXiv :2306.02858  (https://arxiv .org/abs/2306.02858)  [cs.CL  (https://arxiv .org/archive/cs.CL) ].\\n70. OpenAI (2023-03-27). \"GPT -4 Technical Report\". arXiv :2303.08774  (https://arxiv .org/abs/2303.08774)  [cs.CL  (https://arxi\\nv.org/archive/cs.CL) ].\\n71. OpenAI (September 25, 2023). \"GPT -4V(ision) System Card\"  (https://cdn.openai.com/papers/GPTV_System_Card.pdf)\\n(PDF) .\\n72. Pichai, Sundar , Google Keynote (Google I/O \\'23)  (https://www .youtube.com/watch?v=cNfINi5CNbY&t=931s) , timestamp\\n15:31 , retrieved 2023-07-02\\n73. Hoffmann, Jordan; Borgeaud, Sebastian; Mensch, Arthur; Buchatskaya, Elena; Cai, Trevor; Rutherford, Eliza; Casas,\\nDiego de Las; Hendricks, Lisa Anne; W elbl, Johannes; Clark, Aidan; Hennigan, Tom; Noland, Eric; Millican, Katie;\\nDriessche, George van den; Damoc, Bogdan (2022-03-29). \"T raining Compute-Optimal Large Language Models\".\\narXiv :2203.15556  (https://arxiv .org/abs/2203.15556)  [cs.CL  (https://arxiv .org/archive/cs.CL) ].\\n74. Caballero, Ethan; Gupta, Kshitij; Rish, Irina; Krueger , David (2022). \"Broken Neural Scaling Laws\". arXiv :2210.14891  (htt\\nps://arxiv .org/abs/2210.14891)  [cs.LG  (https://arxiv .org/archive/cs.LG) ].\\n75. \"137 emergent abilities of large language models\"  (https://www .jasonwei.net/blog/emergence) . Jason W ei. Retrieved\\n2023-06-24 .\\n76. Hahn, Michael; Goyal, Navin (2023-03-14). \"A  Theory of Emergent In-Context Learning as Implicit Structure Induction\".\\narXiv :2303.07971  (https://arxiv .org/abs/2303.07971)  [cs.LG  (https://arxiv .org/archive/cs.LG) ].\\n77. Pilehvar , Mohammad Taher; Camacho-Collados, Jose (June 2019). \"Proceedings of the 2019 Conference of the North\"\\n(https://aclanthology .org/N19-1 128). Proceedings of the 2019 Conference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human Language T echnologies, V olume 1 (Long and Short Papers) .\\nMinneapolis, Minnesota: Association for Computational Linguistics: 1267–1273. doi:10.18653/v1/N19-1 128 (https://doi.or\\ng/10.18653%2Fv1%2FN19-1 128). S2CID  102353817  (https://api.semanticscholar .org/CorpusID:102353817) .\\n78. \"WiC: The W ord-in-Context Dataset\"  (https://pilehvar .github.io/wic/) . pilehvar .github.io . Retrieved 2023-06-27 .', metadata={'source': 'documents/Large language model - Wikipedia.pdf', 'page': 15}),\n",
              " Document(page_content='1/23/24, 3:23 PM Large language model - Wikipedia\\nhttps://en.wikipedia.org/wiki/Large_language_model 17/2179. Patel, Roma; Pavlick, Ellie (2021-10-06). \"Mapping Language Models to Grounded Conceptual Spaces\"  (https://openrevi\\new.net/forum?id=gJcEM8sxHK) . ICLR .\\n80. A Closer Look at Large Language Models Emergent Abilities  (https://www .notion.so/A-Closer -Look-at-Large-Language-M\\nodels-Emergent-Abilities-493876b55df5479d80686f68a1abd72f)  (Yao Fu, Nov 20, 2022)\\n81. Ornes, Stephen (March 16, 2023). \"The Unpredictable Abilities Emerging From Large AI Models\"  (https://www .quantama\\ngazine.org/the-unpredictable-abilities-emerging-from-large-ai-models-20230316/) . Quanta Magazine .\\n82. Schaef fer, Rylan; Miranda, Brando; Koyejo, Sanmi (2023-04-01). \"Are Emergent Abilities of Large Language Models a\\nMirage?\". arXiv :2304.15004  (https://arxiv .org/abs/2304.15004)  [cs.AI  (https://arxiv .org/archive/cs.AI) ].\\n83. Li, Kenneth; Hopkins, Aspen K.; Bau, David; V iégas, Fernanda; Pfister , Hanspeter; W attenberg, Martin (2022-10-01).\\n\"Emergent W orld Representations: Exploring a Sequence Model Trained on a Synthetic Task\". arXiv :2210.13382  (https://\\narxiv.org/abs/2210.13382)  [cs.LG  (https://arxiv .org/archive/cs.LG) ].\\n84. \"Large Language Model: world models or surface statistics?\"  (https://thegradient.pub/othello/) . The Gradient . 2023-01-21 .\\nRetrieved 2023-06-12 .\\n85. Jin, Charles; Rinard, Martin (2023-05-01). \"Evidence of Meaning in Language Models Trained on Programs\".\\narXiv :2305.1 1169 (https://arxiv .org/abs/2305.1 1169) [cs.LG  (https://arxiv .org/archive/cs.LG) ].\\n86. Nanda, Neel; Chan, Lawrence; Lieberum, Tom; Smith, Jess; Steinhardt, Jacob (2023-01-01). \"Progress measures for\\ngrokking via mechanistic interpretability\". arXiv :2301.05217  (https://arxiv .org/abs/2301.05217)  [cs.LG  (https://arxiv .org/arc\\nhive/cs.LG) ].\\n87. Mitchell, Melanie; Krakauer , David C. (28 March 2023). \"The debate over understanding in AI\\'s large language models\"\\n(https://www .ncbi.nlm.nih.gov/pmc/articles/PMC10068812) . Proceedings of the National Academy of Sciences . 120 (13):\\ne2215907120. arXiv :2210.13966  (https://arxiv .org/abs/2210.13966) . Bibcode :2023PNAS..12015907M  (https://ui.adsabs.h\\narvard.edu/abs/2023PNAS..12015907M) . doi:10.1073/pnas.2215907120  (https://doi.org/10.1073%2Fpnas.2215907120) .\\nPMC  10068812  (https://www .ncbi.nlm.nih.gov/pmc/articles/PMC10068812) . PMID  36943882  (https://pubmed.ncbi.nlm.ni\\nh.gov/36943882) .\\n88. Metz, Cade (16 May 2023). \"Microsoft Says New A.I. Shows Signs of Human Reasoning\"  (https://www .nytimes.com/202\\n3/05/16/technology/microsoft-ai-human-reasoning.html) . The New York T imes .\\n89. Bubeck, Sébastien; Chandrasekaran, V arun; Eldan, Ronen; Gehrke, Johannes; Horvitz, Eric; Kamar , Ece; Lee, Peter;\\nLee, Yin Tat; Li, Yuanzhi; Lundberg, Scott; Nori, Harsha; Palangi, Hamid; Ribeiro, Marco Tulio; Zhang, Yi (2023). \"Sparks\\nof Artificial General Intelligence: Early experiments with GPT -4\". arXiv :2303.12712  (https://arxiv .org/abs/2303.12712)\\n[cs.CL  (https://arxiv .org/archive/cs.CL) ].\\n90. \"ChatGPT  is more like an \\'alien intelligence\\' than a human brain, says futurist\"  (https://www .zdnet.com/article/chatgpt-is-\\nmore-like-an-alien-intelligence-than-a-human-brain-says-futurist/) . ZDNET . 2023 . Retrieved 12 June  2023 .\\n91. Newport, Cal (13 April 2023). \"What Kind of Mind Does ChatGPT  Have?\"  (https://www .newyorker .com/science/annals-of-\\nartificial-intelligence/what-kind-of-mind-does-chatgpt-have) . The New Yorker . Retrieved 12 June  2023 .\\n92. Roose, Kevin (30 May 2023). \"Why an Octopus-like Creature Has Come to Symbolize the State of A.I.\" (https://www .nyti\\nmes.com/2023/05/30/technology/shoggoth-meme-ai.html)  The New York T imes . Retrieved 12 June  2023 .\\n93. \"The A to Z of Artificial Intelligence\"  (https://time.com/6271657/a-to-z-of-artificial-intelligence/) . Time Magazine . 13 April\\n2023 . Retrieved 12 June  2023 .\\n94. Ji, Ziwei; Lee, Nayeon; Frieske, Rita; Yu, Tiezheng; Su, Dan; Xu, Yan; Ishii, Etsuko; Bang, Yejin; Dai, W enliang; Madotto,\\nAndrea; Fung, Pascale (November 2022). \"Survey of Hallucination in Natural Language Generation\"  (https://dl.acm.org/d\\noi/pdf/10.1 145/3571730)  (pdf). ACM Computing Surveys . Association for Computing Machinery . 55 (12): 1–38.\\narXiv :2202.03629  (https://arxiv .org/abs/2202.03629) . doi:10.1145/3571730  (https://doi.org/10.1 145%2F3571730) .\\nS2CID  246652372  (https://api.semanticscholar .org/CorpusID:246652372) . Retrieved 15 January  2023 .\\n95. Varshney , Neeraj (2023). \"A  Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by V alidating\\nLow-Confidence Generation\". arXiv :2307.03987  (https://arxiv .org/abs/2307.03987) . {{cite journal }}: Cite journal\\nrequires |journal=  (help)\\n96. Lakof f, George (1999). Philosophy in the Flesh: The Embodied Mind and Its Challenge to W estern Philosophy; Appendix:\\nThe Neural Theory of Language Paradigm . New York Basic Books. pp. 569–583. ISBN  978-0-465-05674-3 .\\n97. Evans, V yvyan. (2014). The Language Myth . Cambridge University Press. ISBN  978-1-107-04396-1 .\\n98. Friston, Karl J. (2022). Active Inference: The Free Energy Principle in Mind, Brain, and Behavior; Chapter 4 The\\nGenerative Models of Active Inference . The MIT  Press. ISBN  978-0-262-36997-8 .\\n99. Huyen, Chip (2019). \"Understanding Evaluation Metrics for Language Modeling\"  (https://thegradient.pub/understanding-e\\nvaluation-metrics-for-language-models/) . The Gradient . Retrieved January 14,  2024 .\\n100. Clark, Christopher; Lee, Kenton; Chang, Ming-W ei; Kwiatkowski, Tom; Collins, Michael; Toutanova, Kristina (2019).\\n\"BoolQ: Exploring the Surprising Dif ficulty of Natural Yes/No Questions\". arXiv :1905.10044  (https://arxiv .org/abs/1905.100\\n44) [cs.CL  (https://arxiv .org/archive/cs.CL) ].\\n101. Wayne Xin Zhao; Zhou, Kun; Li, Junyi; Tang, Tianyi; W ang, Xiaolei; Hou, Yupeng; Min, Yingqian; Zhang, Beichen; Zhang,\\nJunjie; Dong, Zican; Du, Yifan; Yang, Chen; Chen, Yushuo; Chen, Zhipeng; Jiang, Jinhao; Ren, Ruiyang; Li, Yifan; Tang,\\nXinyu; Liu, Zikang; Liu, Peiyu; Nie, Jian-Y un; W en, Ji-Rong (2023). \"A  Survey of Large Language Models\".\\narXiv :2303.18223  (https://arxiv .org/abs/2303.18223)  [cs.CL  (https://arxiv .org/archive/cs.CL) ].\\n102. Huyen, Chip (18 October 2019). \"Evaluation Metrics for Language Modeling\"  (https://thegradient.pub/understanding-eval\\nuation-metrics-for-language-models/) . The Gradient .\\n103. Srivastava, Aarohi; et al. (2022). \"Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language\\nmodels\". arXiv :2206.04615  (https://arxiv .org/abs/2206.04615)  [cs.CL  (https://arxiv .org/archive/cs.CL) ].', metadata={'source': 'documents/Large language model - Wikipedia.pdf', 'page': 16}),\n",
              " Document(page_content='1/23/24, 3:23 PM Large language model - Wikipedia\\nhttps://en.wikipedia.org/wiki/Large_language_model 18/21104. Lin, Stephanie; Hilton, Jacob; Evans, Owain (2021). \"T ruthfulQA: Measuring How Models Mimic Human Falsehoods\".\\narXiv :2109.07958  (https://arxiv .org/abs/2109.07958)  [cs.CL  (https://arxiv .org/archive/cs.CL) ].\\n105. Zellers, Rowan; Holtzman, Ari; Bisk, Yonatan; Farhadi, Ali; Choi, Yejin (2019). \"HellaSwag: Can a Machine Really Finish\\nYour Sentence?\". arXiv :1905.07830  (https://arxiv .org/abs/1905.07830)  [cs.CL  (https://arxiv .org/archive/cs.CL) ].\\n106. \"Prepare for truly useful large language models\". Nature Biomedical Engineering . 7 (2): 85–86. 7 March 2023.\\ndoi:10.1038/s41551-023-01012-6  (https://doi.org/10.1038%2Fs41551-023-01012-6) . PMID  36882584  (https://pubmed.nc\\nbi.nlm.nih.gov/36882584) . S2CID  257403466  (https://api.semanticscholar .org/CorpusID:257403466) .\\n107. \"Your job is (probably) safe from artificial intelligence\"  (https://www .economist.com/finance-and-economics/2023/05/07/yo\\nur-job-is-probably-safe-from-artificial-intelligence) . The Economist . 7 May 2023 . Retrieved 18 June  2023 .\\n108. \"Generative AI Could Raise Global GDP  by 7%\"  (https://www .goldmansachs.com/intelligence/pages/generative-ai-could-r\\naise-global-gdp-by-7-percent.html) . Goldman Sachs . Retrieved 18 June  2023 .\\n109. Peng, Zhencan; W ang, Zhizhi; Deng, Dong (13 June 2023). \"Near-Duplicate Sequence Search at Scale for Large\\nLanguage Model Memorization Evaluation\"  (https://people.cs.rutgers.edu/~dd903/assets/papers/sigmod23.pdf)  (PDF) .\\nProceedings of the ACM on Management of Data . 1 (2): 1–18. doi:10.1145/3589324  (https://doi.org/10.1 145%2F358932\\n4). S2CID  259213212  (https://api.semanticscholar .org/CorpusID:259213212) . Retrieved 2024-01-20 . Citing Lee et al\\n2022.\\n110. Peng, W ang & Deng 2023 , p. 8.\\n111. Alba, Davey (1 May 2023). \"AI chatbots have been used to create dozens of news content farms\"  (https://www .japantime\\ns.co.jp/news/2023/05/01/business/tech/ai-fake-news-content-farms/) . The Japan T imes . Retrieved 18 June  2023 .\\n112. \"Could chatbots help devise the next pandemic virus?\"  (https://www .science.org/content/article/could-chatbots-help-devis\\ne-next-pandemic-virus) . Science . 14 June 2023. doi:10.1126/science.adj2463  (https://doi.org/10.1 126%2Fscience.adj246\\n3).\\n113. Stephen Council (1 Dec 2023). \"How Googlers cracked an SF rival\\'s tech model with a single word\"  (https://www .sfgate.c\\nom/tech/article/google-openai-chatgpt-break-model-18525445.php) . SFGA TE.\\n114. Hubinger , Evan (10 January 2024). \"Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training\".\\narXiv :2401.05566  (https://arxiv .org/abs/2401.05566)  [cs.CR  (https://arxiv .org/archive/cs.CR) ].\\n115. Stokel-W alker , Chris (November 22, 2023). \"ChatGPT  Replicates Gender Bias in Recommendation Letters\"  (https://www .\\nscientificamerican.com/article/chatgpt-replicates-gender-bias-in-recommendation-letters/) . Scientific American . Retrieved\\n2023-12-29 .\\n116. Luo, Queenie; Puett, Michael J.; Smith, Michael D. (2023-03-28). \"A  Perspectival Mirror of the Elephant: Investigating\\nLanguage Bias on Google, ChatGPT , Wikipedia, and YouTube\". arXiv :2303.16281v2  (https://arxiv .org/abs/2303.16281v2)\\n[cs.CY  (https://arxiv .org/archive/cs.CY) ].\\n117. Cheng, Myra; Durmus, Esin; Jurafsky , Dan (2023-05-29), Marked Personas: Using Natural Language Prompts to\\nMeasure Stereotypes in Language Models , arXiv :2305.18189  (https://arxiv .org/abs/2305.18189)\\n118. Kotek, Hadas; Dockum, Rikker; Sun, David (2023-1 1-05). \"Gender bias and stereotypes in Large Language Models\"  (http\\ns://dl.acm.org/doi/10.1 145/3582269.3615599) . Proceedings of the ACM Collective Intelligence Conference . CI \\'23. New\\nYork, NY , USA: Association for Computing Machinery . pp. 12–24. doi:10.1145/3582269.3615599  (https://doi.org/10.1 14\\n5%2F3582269.3615599) . ISBN  979-8-4007-01 13-9.\\n119. Heikkilä, Melissa (August 7, 2023). \"AI language models are rife with dif ferent political biases\"  (https://www .technologyrev\\niew.com/2023/08/07/1077324/ai-language-models-are-rife-with-political-biases/) . MIT T echnology Review . Retrieved\\n2023-12-29 .\\n120. \"finetune-transformer-lm\"  (https://github.com/openai/finetune-transformer-lm) . GitHub . Retrieved 2 January  2024 .\\n121. Devlin, Jacob; Chang, Ming-W ei; Lee, Kenton; Toutanova, Kristina (1 1 October 2018). \"BER T: Pre-training of Deep\\nBidirectional Transformers for Language Understanding\". arXiv :1810.04805v2  (https://arxiv .org/abs/1810.04805v2)\\n[cs.CL  (https://arxiv .org/archive/cs.CL) ].\\n122. Prickett, Nicole Hemsoth (2021-08-24). \"Cerebras Shifts Architecture To Meet Massive AI/ML  Models\"  (https://www .nextpl\\natform.com/2021/08/24/cerebras-shifts-architecture-to-meet-massive-ai-ml-models/) . The Next Platform . Retrieved\\n2023-06-20 .\\n123. \"BER T\" (https://github.com/google-research/bert) . March 13, 2023 – via GitHub.\\n124. Patel, Ajay; Li, Bryan; Rasooli, Mohammad Sadegh; Constant, Noah; Raf fel, Colin; Callison-Burch, Chris (2022).\\n\"Bidirectional Language Models Are Also Few-shot Learners\". arXiv :2209.14500  (https://arxiv .org/abs/2209.14500)\\n[cs.LG  (https://arxiv .org/archive/cs.LG) ].\\n125. \"BER T, RoBER Ta, DistilBER T, XLNet: Which one to use?\"  (https://www .kdnuggets.com/bert-roberta-distilbert-xlnet-which-\\none-to-use.html) . KDnuggets .\\n126. \"xlnet\"  (https://github.com/zihangdai/xlnet/) . GitHub . Retrieved 2 January  2024 .\\n127. Naik, Amit Raja (September 23, 2021). \"Google Introduces New Architecture To Reduce Cost Of Transformers\"  (https://a\\nnalyticsindiamag.com/google-introduces-new-architecture-to-reduce-cost-of-transformers/) . Analytics India Magazine .\\n128. Yang, Zhilin; Dai, Zihang; Yang, Yiming; Carbonell, Jaime; Salakhutdinov , Ruslan; Le, Quoc V . (2 January 2020). \"XLNet:\\nGeneralized Autoregressive Pretraining for Language Understanding\". arXiv :1906.08237  (https://arxiv .org/abs/1906.0823\\n7) [cs.CL  (https://arxiv .org/archive/cs.CL) ].\\n129. \"GPT -2: 1.5B Release\"  (https://openai.com/blog/gpt-2-1-5b-release/) . OpenAI . 2019-1 1-05. Archived  (https://web.archive.\\norg/web/20191 114074358/https://openai.com/blog/gpt-2-1-5b-release/)  from the original on 2019-1 1-14. Retrieved\\n2019-1 1-14.\\n130. \"Better language models and their implications\"  (https://openai.com/research/better-language-models) . openai.com .', metadata={'source': 'documents/Large language model - Wikipedia.pdf', 'page': 17}),\n",
              " Document(page_content='1/23/24, 3:23 PM Large language model - Wikipedia\\nhttps://en.wikipedia.org/wiki/Large_language_model 19/21131. \"OpenAI\\'s GPT -3 Language Model: A Technical Overview\"  (https://lambdalabs.com/blog/demystifying-gpt-3) .\\nlambdalabs.com . 3 June 2020.\\n132. \"gpt-2\"  (https://github.com/openai/gpt-2) . GitHub . Retrieved 13 March  2023 .\\n133. Table D.1 in Brown, Tom B.; Mann, Benjamin; Ryder , Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla;\\nNeelakantan, Arvind; Shyam, Pranav; Sastry , Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-V oss, Ariel; Krueger ,\\nGretchen; Henighan, Tom; Child, Rewon; Ramesh, Aditya; Ziegler , Daniel M.; W u, Jef frey; Winter , Clemens; Hesse,\\nChristopher; Chen, Mark; Sigler , Eric; Litwin, Mateusz; Gray , Scott; Chess, Benjamin; Clark, Jack; Berner , Christopher;\\nMcCandlish, Sam; Radford, Alec; Sutskever , Ilya; Amodei, Dario (May 28, 2020). \"Language Models are Few-Shot\\nLearners\". arXiv :2005.14165v4  (https://arxiv .org/abs/2005.14165v4)  [cs.CL  (https://arxiv .org/archive/cs.CL) ].\\n134. \"ChatGPT : Optimizing Language Models for Dialogue\"  (https://openai.com/blog/chatgpt/) . OpenAI . 2022-1 1-30. Retrieved\\n2023-01-13 .\\n135. \"GPT  Neo\"  (https://github.com/EleutherAI/gpt-neo) . March 15, 2023 – via GitHub.\\n136. Gao, Leo; Biderman, Stella; Black, Sid; Golding, Laurence; Hoppe, Travis; Foster , Charles; Phang, Jason; He, Horace;\\nThite, Anish; Nabeshima, Noa; Presser , Shawn; Leahy , Connor (31 December 2020). \"The Pile: An 800GB Dataset of\\nDiverse Text for Language Modeling\". arXiv :2101.00027  (https://arxiv .org/abs/2101.00027)  [cs.CL  (https://arxiv .org/archiv\\ne/cs.CL) ].\\n137. Iyer, Abhishek (15 May 2021). \"GPT -3\\'s free alternative GPT -Neo is something to be excited about\"  (https://venturebeat.c\\nom/ai/gpt-3s-free-alternative-gpt-neo-is-something-to-be-excited-about/) . VentureBeat .\\n138. \"GPT -J-6B: An Introduction to the Largest Open Source GPT  Model | Forefront\"  (https://www .forefront.ai/blog-posts/gpt-j-\\n6b-an-introduction-to-the-largest-open-sourced-gpt-model) . www .forefront.ai . Retrieved 2023-02-28 .\\n139. Dey, Nolan; Gosal, Gurpreet; Zhiming; Chen; Khachane, Hemant; Marshall, William; Pathria, Ribhu; Tom, Marvin;\\nHestness, Joel (2023-04-01). \"Cerebras-GPT : Open Compute-Optimal Language Models Trained on the Cerebras W afer-\\nScale Cluster\". arXiv :2304.03208  (https://arxiv .org/abs/2304.03208)  [cs.LG  (https://arxiv .org/archive/cs.LG) ].\\n140. Alvi, Ali; Kharya, Paresh (1 1 October 2021). \"Using DeepSpeed and Megatron to Train Megatron-T uring NLG 530B, the\\nWorld\\'s Largest and Most Powerful Generative Language Model\"  (https://www .microsoft.com/en-us/research/blog/using-d\\neepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-m\\nodel/) . Microsoft Research .\\n141. Smith, Shaden; Patwary , Mostofa; Norick, Brandon; LeGresley , Patrick; Rajbhandari, Samyam; Casper , Jared; Liu, Zhun;\\nPrabhumoye, Shrimai; Zerveas, George; Korthikanti, V ijay; Zhang, Elton; Child, Rewon; Aminabadi, Reza Yazdani;\\nBernauer , Julie; Song, Xia (2022-02-04). \"Using DeepSpeed and Megatron to Train Megatron-T uring NLG 530B, A Large-\\nScale Generative Language Model\". arXiv :2201.1 1990  (https://arxiv .org/abs/2201.1 1990)  [cs.CL  (https://arxiv .org/archive/\\ncs.CL) ].\\n142. Wang, Shuohuan; Sun, Yu; Xiang, Yang; W u, Zhihua; Ding, Siyu; Gong, W eibao; Feng, Shikun; Shang, Junyuan; Zhao,\\nYanbin; Pang, Chao; Liu, Jiaxiang; Chen, Xuyi; Lu, Yuxiang; Liu, W eixin; W ang, Xi; Bai, Yangfan; Chen, Qiuliang; Zhao,\\nLi; Li, Shiyong; Sun, Peng; Yu, Dianhai; Ma, Yanjun; Tian, Hao; W u, Hua; W u, Tian; Zeng, W ei; Li, Ge; Gao, W en; W ang,\\nHaifeng (December 23, 2021). \"ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language\\nUnderstanding and Generation\". arXiv :2112.12731  (https://arxiv .org/abs/21 12.12731)  [cs.CL  (https://arxiv .org/archive/cs.\\nCL)].\\n143. \"Product\"  (https://www .anthropic.com/product) . Anthropic . Retrieved 14 March  2023 .\\n144. Askell, Amanda; Bai, Yuntao; Chen, Anna; et al. (9 December 2021). \"A  General Language Assistant as a Laboratory for\\nAlignment\". arXiv :2112.00861  (https://arxiv .org/abs/21 12.00861)  [cs.CL  (https://arxiv .org/archive/cs.CL) ].\\n145. Bai, Yuntao; Kadavath, Saurav; Kundu, Sandipan; et al. (15 December 2022). \"Constitutional AI: Harmlessness from AI\\nFeedback\". arXiv :2212.08073  (https://arxiv .org/abs/2212.08073)  [cs.CL  (https://arxiv .org/archive/cs.CL) ].\\n146. \"Language modelling at scale: Gopher , ethical considerations, and retrieval\"  (https://www .deepmind.com/blog/language-\\nmodelling-at-scale-gopher-ethical-considerations-and-retrieval) . www .deepmind.com . 8 December 2021 . Retrieved\\n20 March  2023 .\\n147. Hoffmann, Jordan; Borgeaud, Sebastian; Mensch, Arthur; et al. (29 March 2022). \"T raining Compute-Optimal Large\\nLanguage Models\". arXiv :2203.15556  (https://arxiv .org/abs/2203.15556)  [cs.CL  (https://arxiv .org/archive/cs.CL) ].\\n148. Table 20 and page 66 of PaLM: Scaling Language Modeling with Pathways  (https://storage.googleapis.com/pathways-lan\\nguage-model/PaLM-paper .pdf)\\n149. Cheng, Heng-Tze; Thoppilan, Romal (January 21, 2022). \"LaMDA: Towards Safe, Grounded, and High-Quality Dialog\\nModels for Everything\"  (https://ai.googleblog.com/2022/01/lamda-towards-safe-grounded-and-high.html) .\\nai.googleblog.com . Retrieved 2023-03-09 .\\n150. Thoppilan, Romal; De Freitas, Daniel; Hall, Jamie; Shazeer , Noam; Kulshreshtha, Apoorv; Cheng, Heng-Tze; Jin, Alicia;\\nBos, Taylor; Baker , Leslie; Du, Yu; Li, YaGuang; Lee, Hongrae; Zheng, Huaixiu Steven; Ghafouri, Amin; Menegali,\\nMarcelo (2022-01-01). \"LaMDA: Language Models for Dialog Applications\". arXiv :2201.08239  (https://arxiv .org/abs/2201.\\n08239)  [cs.CL  (https://arxiv .org/archive/cs.CL) ].\\n151. Black, Sidney; Biderman, Stella; Hallahan, Eric; et al. (2022-05-01). GPT-NeoX-20B: An Open-Source Autoregressive\\nLanguage Model  (https://aclanthology .org/2022.bigscience-1.9/) . Proceedings of BigScience Episode #5 -- W orkshop on\\nChallenges & Perspectives in Creating Large Language Models. V ol. Proceedings of BigScience Episode #5 -- W orkshop\\non Challenges & Perspectives in Creating Large Language Models. pp. 95–136 . Retrieved 2022-12-19 .\\n152. Hoffmann, Jordan; Borgeaud, Sebastian; Mensch, Arthur; Sifre, Laurent (12 April 2022). \"An empirical analysis of\\ncompute-optimal large language model training\"  (https://www .deepmind.com/blog/an-empirical-analysis-of-compute-opti\\nmal-large-language-model-training) . Deepmind Blog .', metadata={'source': 'documents/Large language model - Wikipedia.pdf', 'page': 18}),\n",
              " Document(page_content='1/23/24, 3:23 PM Large language model - Wikipedia\\nhttps://en.wikipedia.org/wiki/Large_language_model 20/21153. Narang, Sharan; Chowdhery , Aakanksha (April 4, 2022). \"Pathways Language Model (PaLM): Scaling to 540 Billion\\nParameters for Breakthrough Performance\"  (https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-t\\no.html) . ai.googleblog.com . Retrieved 2023-03-09 .\\n154. \"Democratizing access to large-scale language models with OPT -175B\"  (https://ai.facebook.com/blog/democratizing-acc\\ness-to-large-scale-language-models-with-opt-175b/) . ai.facebook.com .\\n155. Zhang, Susan; Roller , Stephen; Goyal, Naman; Artetxe, Mikel; Chen, Moya; Chen, Shuohui; Dewan, Christopher; Diab,\\nMona; Li, Xian; Lin, Xi V ictoria; Mihaylov , Todor; Ott, Myle; Shleifer , Sam; Shuster , Kurt; Simig, Daniel; Koura, Punit\\nSingh; Sridhar , Anjali; W ang, Tianlu; Zettlemoyer , Luke (21 June 2022). \"OPT : Open Pre-trained Transformer Language\\nModels\". arXiv :2205.01068  (https://arxiv .org/abs/2205.01068)  [cs.CL  (https://arxiv .org/archive/cs.CL) ].\\n156. Khrushchev , Mikhail; V asilev , Ruslan; Petrov , Alexey; Zinov , Nikolay (2022-06-22), YaLM 100B  (https://github.com/yande\\nx/YaLM-100B) , retrieved 2023-03-18\\n157. Lewkowycz, Aitor; Andreassen, Anders; Dohan, David; Dyer , Ethan; Michalewski, Henryk; Ramasesh, V inay; Slone,\\nAmbrose; Anil, Cem; Schlag, Imanol; Gutman-Solo, Theo; W u, Yuhuai; Neyshabur , Behnam; Gur-Ari, Guy; Misra, V edant\\n(30 June 2022). \"Solving Quantitative Reasoning Problems with Language Models\". arXiv :2206.14858  (https://arxiv .org/a\\nbs/2206.14858)  [cs.CL  (https://arxiv .org/archive/cs.CL) ].\\n158. \"Minerva: Solving Quantitative Reasoning Problems with Language Models\"  (https://ai.googleblog.com/2022/06/minerva-\\nsolving-quantitative-reasoning.html) . ai.googleblog.com . 30 June 2022 . Retrieved 20 March  2023 .\\n159. Ananthaswamy , Anil (8 March 2023). \"In AI, is bigger always better?\"  (https://www .nature.com/articles/d41586-023-00641\\n-w). Nature . 615 (7951): 202–205. Bibcode :2023Natur .615..202A  (https://ui.adsabs.harvard.edu/abs/2023Natur .615..202\\nA). doi:10.1038/d41586-023-00641-w  (https://doi.org/10.1038%2Fd41586-023-00641-w) . PMID  36890378  (https://pubme\\nd.ncbi.nlm.nih.gov/36890378) . S2CID  257380916  (https://api.semanticscholar .org/CorpusID:257380916) .\\n160. \"bigscience/bloom · Hugging Face\"  (https://huggingface.co/bigscience/bloom) . huggingface.co .\\n161. Taylor , Ross; Kardas, Marcin; Cucurull, Guillem; Scialom, Thomas; Hartshorn, Anthony; Saravia, Elvis; Poulton, Andrew;\\nKerkez, V iktor; Stojnic, Robert (16 November 2022). \"Galactica: A Large Language Model for Science\". arXiv :2211.09085\\n(https://arxiv .org/abs/221 1.09085)  [cs.CL  (https://arxiv .org/archive/cs.CL) ].\\n162. \"20B-parameter Alexa model sets new marks in few-shot learning\"  (https://www .amazon.science/blog/20b-parameter-ale\\nxa-model-sets-new-marks-in-few-shot-learning) . Amazon Science . 2 August 2022.\\n163. Soltan, Saleh; Ananthakrishnan, Shankar; FitzGerald, Jack; et al. (3 August 2022). \"AlexaTM 20B: Few-Shot Learning\\nUsing a Large-Scale Multilingual Seq2Seq Model\". arXiv :2208.01448  (https://arxiv .org/abs/2208.01448)  [cs.CL  (https://ar\\nxiv.org/archive/cs.CL) ].\\n164. \"AlexaTM 20B is now available in Amazon SageMaker JumpStart | AWS Machine Learning Blog\"  (https://aws.amazon.co\\nm/blogs/machine-learning/alexatm-20b-is-now-available-in-amazon-sagemaker-jumpstart/) . aws.amazon.com . 17\\nNovember 2022 . Retrieved 13 March  2023 .\\n165. \"Introducing LLaMA: A foundational, 65-billion-parameter large language model\"  (https://ai.facebook.com/blog/large-langu\\nage-model-llama-meta-ai/) . Meta AI. 24 February 2023.\\n166. \"The Falcon has landed in the Hugging Face ecosystem\"  (https://huggingface.co/blog/falcon) . huggingface.co . Retrieved\\n2023-06-20 .\\n167. \"Stanford CRFM\"  (https://crfm.stanford.edu/2023/03/13/alpaca.html) . crfm.stanford.edu .\\n168. \"GPT -4 Technical Report\"  (https://cdn.openai.com/papers/gpt-4.pdf)  (PDF) . OpenAI . 2023. Archived  (https://web.archive.\\norg/web/20230314190904/https://cdn.openai.com/papers/gpt-4.pdf)  (PDF)  from the original on March 14, 2023 .\\nRetrieved March 14,  2023 .\\n169. Dey, Nolan (March 28, 2023). \"Cerebras-GPT : A Family of Open, Compute-ef ficient, Large Language Models\"  (https://ww\\nw.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-ef ficient-large-language-models/) . Cerebras .\\n170. \"Abu Dhabi-based TII launches its own version of ChatGPT\"  (https://fastcompanyme.com/news/abu-dhabi-based-tii-launc\\nhes-its-own-version-of-chatgpt/) . tii.ae .\\n171. Penedo, Guilherme; Malartic, Quentin; Hesslow , Daniel; Cojocaru, Ruxandra; Cappelli, Alessandro; Alobeidli, Hamza;\\nPannier , Baptiste; Almazrouei, Ebtesam; Launay , Julien (2023-06-01). \"The RefinedW eb Dataset for Falcon LLM:\\nOutperforming Curated Corpora with W eb Data, and W eb Data Only\". arXiv :2306.01 116 (https://arxiv .org/abs/2306.01 11\\n6) [cs.CL  (https://arxiv .org/archive/cs.CL) ].\\n172. \"tiiuae/falcon-40b · Hugging Face\"  (https://huggingface.co/tiiuae/falcon-40b) . huggingface.co . 2023-06-09 . Retrieved\\n2023-06-20 .\\n173. UAE’ s Falcon 40B, W orld’s Top-Ranked AI Model from Technology Innovation Institute, is Now Royalty-Free  (https://www .\\nbusinesswire.com/news/home/20230531005608/en/UAE’ s-Falcon-40B-W orld’s-Top-Ranked-AI-Model-from-T echnology-I\\nnnovation-Institute-is-Now-Royalty-Free) , 31 May 2023\\n174. Wu, Shijie; Irsoy , Ozan; Lu, Steven; Dabravolski, V adim; Dredze, Mark; Gehrmann, Sebastian; Kambadur , Prabhanjan;\\nRosenberg, David; Mann, Gideon (March 30, 2023). \"BloombergGPT : A Large Language Model for Finance\".\\narXiv :2303.17564  (https://arxiv .org/abs/2303.17564)  [cs.LG  (https://arxiv .org/archive/cs.LG) ].\\n175. Ren, Xiaozhe; Zhou, Pingyi; Meng, Xinfan; Huang, Xinjing; W ang, Yadao; W ang, W eichao; Li, Pengfei; Zhang, Xiaoda;\\nPodolskiy , Alexander; Arshinov , Grigory; Bout, Andrey; Piontkovskaya, Irina; W ei, Jiansheng; Jiang, Xin; Su, Teng; Liu,\\nQun; Yao, Jun (March 19, 2023). \"PanGu-Σ: T owards Trillion Parameter Language Model with Sparse Heterogeneous\\nComputing\". arXiv :2303.10845  (https://arxiv .org/abs/2303.10845)  [cs.CL  (https://arxiv .org/archive/cs.CL) ].\\n176. Köpf, Andreas; Kilcher , Yannic; von Rütte, Dimitri; Anagnostidis, Sotiris; Tam, Zhi-Rui; Stevens, Keith; Barhoum, Abdullah;\\nDuc, Nguyen Minh; Stanley , Oliver; Nagyfi, Richárd; ES, Shahul; Suri, Sameer; Glushkov , David; Dantuluri, Arnav;\\nMaguire, Andrew (2023-04-14). \"OpenAssistant Conversations -- Democratizing Large Language Model Alignment\".\\narXiv :2304.07327  (https://arxiv .org/abs/2304.07327)  [cs.CL  (https://arxiv .org/archive/cs.CL) ].', metadata={'source': 'documents/Large language model - Wikipedia.pdf', 'page': 19}),\n",
              " Document(page_content='1/23/24, 3:23 PM Large language model - Wikipedia\\nhttps://en.wikipedia.org/wiki/Large_language_model 21/21177. Wrobel, Sharon. \"Tel Aviv startup rolls out new advanced AI language model to rival OpenAI\"  (https://www .timesofisrael.c\\nom/ai21-labs-rolls-out-new-advanced-ai-language-model-to-rival-openai/) . www .timesofisrael.com . Retrieved 2023-07-24 .\\n178. Wiggers, Kyle (2023-04-13). \"With Bedrock, Amazon enters the generative AI race\"  (https://techcrunch.com/2023/04/13/w\\nith-bedrock-amazon-enters-the-generative-ai-race/) . TechCrunch . Retrieved 2023-07-24 .\\n179. Elias, Jennifer (16 May 2023). \"Google\\'s newest A.I. model uses nearly five times more text data for training than its\\npredecessor\"  (https://www .cnbc.com/2023/05/16/googles-palm-2-uses-nearly-five-times-more-text-data-than-predecesso\\nr.html) . CNBC . Retrieved 18 May  2023 .\\n180. \"Introducing PaLM 2\"  (https://blog.google/technology/ai/google-palm-2-ai-large-language-model/) . Google . May 10, 2023.\\n181. \"Introducing Llama 2: The Next Generation of Our Open Source Large Language Model\"  (https://ai.meta.com/llama/) .\\nMeta AI. 2023 . Retrieved 2023-07-19 .\\n182. \"Claude 2\"  (https://www .anthropic.com/index/claude-2) . anthropic.com . Retrieved 12 December  2023 .\\n183. \"Falcon 180B\"  (https://falconllm.tii.ae/falcon-180b.html) . Technology Innovation Institute . 2023 . Retrieved 2023-09-21 .\\n184. \"Announcing Mistral 7B\"  (https://mistral.ai/news/announcing-mistral-7b/) . Mistral . 2023 . Retrieved 2023-10-06 .\\n185. \"Introducing Claude 2.1\"  (https://www .anthropic.com/index/claude-2-1) . anthropic.com . Retrieved 12 December  2023 .\\n186. \"Grok-1 model card\"  (https://x.ai/model-card/) . x.ai. Retrieved 12 December  2023 .\\n187. \"Gemini - Google DeepMind\"  (https://deepmind.google/technologies/gemini/#capabilities) . deepmind.google . Retrieved\\n12 December  2023 .\\n188. \"Mixtral of experts\"  (https://mistral.ai/news/mixtral-of-experts/) . mistral.ai . 11 December 2023 . Retrieved 12 December\\n2023 .\\n189. Franzen, Carl (1 1 December 2023). \"Mistral shocks AI community as latest open source model eclipses GPT -3.5\\nperformance\"  (https://venturebeat.com/ai/mistral-shocks-ai-community-as-latest-open-source-model-eclipses-gpt-3-5-perf\\normance/) . VentureBeat . Retrieved 12 December  2023 .\\n190. Hughes, Alyssa (12 December 2023). \"Phi-2: The surprising power of small language models\"  (https://www .microsoft.co\\nm/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/) . Microsoft Research . Retrieved\\n13 December  2023 .\\nJurafsky , Dan , Martin, James. H. Speech and Language Processing: An Introduction to Natural Language Processing,\\nComputational Linguistics, and Speech Recognition  (https://web.stanford.edu/~jurafsky/slp3/ed3book_jan72023.pdf) , 3rd\\nEdition draft, 2023.\\nPhuong, Mary; Hutter , Marcus (2022). \"Formal Algorithms for Transformers\". arXiv :2207.09238  (https://arxiv .org/abs/220\\n7.09238)  [cs.LG  (https://arxiv .org/archive/cs.LG) ].\\nEloundou, Tyna; Manning, Sam; Mishkin, Pamela; Rock, Daniel (2023). \"GPT s are GPT s: An Early Look at the Labor\\nMarket Impact Potential of Large Language Models\". arXiv :2303.10130  (https://arxiv .org/abs/2303.10130)  [econ.GN  (http\\ns://arxiv .org/archive/econ.GN) ].\\nEldan, Ronen; Li, Yuanzhi (2023). \"T inyStories: How Small Can Language Models Be and Still Speak Coherent\\nEnglish?\". arXiv :2305.07759  (https://arxiv .org/abs/2305.07759)  [cs.CL  (https://arxiv .org/archive/cs.CL) ].\\nFrank, Michael C. (27 June 2023). \"Baby steps in evaluating the capacities of large language models\"  (https://www .natur\\ne.com/articles/s44159-023-0021 1-x). Nature Reviews Psychology . 2 (8): 451–452. doi:10.1038/s44159-023-0021 1-x (http\\ns://doi.org/10.1038%2Fs44159-023-0021 1-x). ISSN  2731-0574  (https://www .worldcat.org/issn/2731-0574) .\\nS2CID  259713140  (https://api.semanticscholar .org/CorpusID:259713140) . Retrieved 2 July  2023 .\\nZhao, W ayne Xin; et al. (2023). \"A  Survey of Large Language Models\". arXiv :2303.18223  (https://arxiv .org/abs/2303.1822\\n3) [cs.CL  (https://arxiv .org/archive/cs.CL) ].\\nKaddour , Jean; et al. (2023). \"Challenges and Applications of Large Language Models\". arXiv :2307.10169  (https://arxiv .or\\ng/abs/2307.10169)  [cs.CL  (https://arxiv .org/archive/cs.CL) ].\\nYin, Shukang; Fu, Chaoyou; Zhao, Sirui; Li, Ke; Sun, Xing; Xu, Tong; Chen, Enhong (2023-06-01). \"A  Survey on\\nMultimodal Large Language Models\". arXiv :2306.13549  (https://arxiv .org/abs/2306.13549)  [cs.CV  (https://arxiv .org/archiv\\ne/cs.CV) ].\\nOpen LLMs repository  (https://github.com/eugeneyan/open-llms)  on GitHub .\\nRetrieved from \"https://en.wikipedia.org/w/index.php?title=Large_language_model&oldid=1198170114\"Further reading', metadata={'source': 'documents/Large language model - Wikipedia.pdf', 'page': 20})]"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "chunks = text_splitter.split_documents(data)"
      ],
      "metadata": {
        "id": "vtSd5ZjEwr_E"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Weaviate\n",
        "import weaviate\n",
        "from weaviate.embedded import EmbeddedOptions\n",
        "\n",
        "client = weaviate.Client(\n",
        "  embedded_options = EmbeddedOptions()\n",
        ")\n",
        "\n",
        "vectorstore = Weaviate.from_documents(\n",
        "    client = client,\n",
        "    documents = chunks,\n",
        "    embedding = OpenAIEmbeddings(api_key= OPENAI_API_KEY),\n",
        "    by_text = False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXNXHd0cxHFG",
        "outputId": "c6f52f3a-45a8-4d85-b794-03cd9962bdec"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embedded weaviate is already listening on port 8079\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "x9tUQYtQxRfo"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "template = \"\"\"You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrieved context to answer the question.\n",
        "If you don't know the answer, just say that you don't know.\n",
        "Use three sentences maximum and keep the answer concise.\n",
        "Question: {question}\n",
        "Context: {context}\n",
        "Answer:\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dco2kooixzCA",
        "outputId": "8d48ef5b-3474-4681-86fe-49b6139ac6e6"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variables=['context', 'question'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. \\nUse the following pieces of retrieved context to answer the question. \\nIf you don't know the answer, just say that you don't know. \\nUse three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\\n\"))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "llm = ChatOpenAI(api_key= OPENAI_API_KEY, model_name=\"gpt-3.5-turbo-16k\", temperature=0)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever,  \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "query = \"Please tell me the latest LLM models and the year of release\"\n",
        "print(rag_chain.invoke(query))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhKafpdhDPlw",
        "outputId": "d581077a-6925-41eb-86cd-d0fe0e6cc0b6"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The latest LLM models and their release years are as follows:\n",
            "1. LaMDA - January 2022\n",
            "2. GPT-NeoX - February 2022\n",
            "3. Chinchilla - March 2022\n",
            "4. PaLM (Pathways Language Model) - April 2022\n",
            "5. OPT (Open Pretrained Transformer) - May 2022\n",
            "6. YaLM 100B - June 2022\n",
            "7. Minerva - June 2022\n",
            "8. BLOOM - July 2022\n",
            "9. Galactica - November 2022\n",
            "10. AlexaTM (Teacher Models) - November 2022\n",
            "11. LLaMA (Large Language Model Meta AI) - February 2023\n",
            "12. GPT-4 - March 2023\n",
            "13. Cerebras-GPT - March 2023\n",
            "14. Falcon - March 2023\n",
            "15. BloombergGPT - March 2023\n",
            "16. PanGu-Σ - March 2023\n",
            "17. OpenAssistant - March 2023\n",
            "18. Claude 2 - November 2023\n",
            "19. Grok-1 - November 2023\n",
            "20. Gemini - December 2023\n",
            "21. Mixtral 8x7B - December 2023\n",
            "22. Phi-2 - December 2023\n",
            "23. Foundation models - Release dates not specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "conhyRU5CKqH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}